{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Insurance cost prediction using linear regression\n",
    "\n",
    "In this assignment we're going to use information like a person's age, sex, BMI, no. of children and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from: https://www.kaggle.com/mirichoi0218/insurance\n",
    "\n",
    "\n",
    "We will create a model with the following steps:\n",
    "1. Download and explore the dataset\n",
    "2. Prepare the dataset for training\n",
    "3. Create a linear regression model\n",
    "4. Train the model to fit the data\n",
    "5. Make predictions using the trained model\n",
    "\n",
    "\n",
    "This assignment builds upon the concepts from the first 2 lectures. It will help to review these Jupyter notebooks:\n",
    "- PyTorch basics: https://jovian.ml/aakashns/01-pytorch-basics\n",
    "- Linear Regression: https://jovian.ml/aakashns/02-linear-regression\n",
    "- Logistic Regression: https://jovian.ml/aakashns/03-logistic-regression\n",
    "- Linear regression (minimal): https://jovian.ml/aakashns/housing-linear-minimal\n",
    "- Logistic regression (minimal): https://jovian.ml/aakashns/mnist-logistic-minimal\n",
    "\n",
    "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the commands below if imports fail\n",
    "# !conda install numpy pytorch torchvision cpuonly -c pytorch -y\n",
    "# !pip install matplotlib --upgrade --quiet\n",
    "# !pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import jovian\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='02-insurance-linear-regression' # will be used by jovian.commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and explore the data\n",
    "\n",
    "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\insurance.csv\n"
     ]
    }
   ],
   "source": [
    "# Commenting this code because file is already downloaded\n",
    "DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n",
    "download_url(DATASET_URL, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_FILENAME = \"insurance.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = \"saurabhjain\" # at least 5 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_dataset(dataframe_raw, rand_str):\n",
    "    dataframe = dataframe_raw.copy(deep=True)\n",
    "    # drop some rows\n",
    "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
    "    # scale input\n",
    "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n",
    "    # scale target\n",
    "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n",
    "    # drop column\n",
    "    if ord(rand_str[3]) % 2 == 1:\n",
    "        dataframe = dataframe.drop(['region'], axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "      <td>33.81905</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>3392.402539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>20</td>\n",
       "      <td>male</td>\n",
       "      <td>21.34000</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "      <td>2298.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>28.01360</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>5075.150184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>39</td>\n",
       "      <td>male</td>\n",
       "      <td>41.37535</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>6736.173736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>38</td>\n",
       "      <td>female</td>\n",
       "      <td>36.59810</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>6315.211539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex       bmi  children smoker     region      charges\n",
       "1178   23  female  33.81905         0     no  northeast  3392.402539\n",
       "1295   20    male  21.34000         1     no  southwest  2298.792600\n",
       "205    28  female  28.01360         1     no  northeast  5075.150184\n",
       "1067   39    male  41.37535         0     no  northeast  6736.173736\n",
       "523    38  female  36.59810         0     no  southeast  6315.211539"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = customize_dataset(dataframe_raw, your_name)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us answer some basic questions about the dataset. \n",
    "\n",
    "\n",
    "**Q: How many rows does the dataset have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n"
     ]
    }
   ],
   "source": [
    "num_rows = dataframe.shape[0]\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How many columns doe the dataset have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "num_cols = dataframe.shape[1]\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of the input variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1271 entries, 1178 to 604\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1271 non-null   int64  \n",
      " 1   sex       1271 non-null   object \n",
      " 2   bmi       1271 non-null   float64\n",
      " 3   children  1271 non-null   int64  \n",
      " 4   smoker    1271 non-null   object \n",
      " 5   region    1271 non-null   object \n",
      " 6   charges   1271 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 79.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'children', 'smoker', 'region']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Which of the input columns are non-numeric or categorial variables ?**\n",
    "\n",
    "Hint: `sex` is one of them. List the columns that are not numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'smoker', 'region']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = ['sex', 'smoker', 'region']\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of output/target variable(s)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charges']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cols = ['charges']\n",
    "output_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n",
    "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1271.000000\n",
       "mean     15579.435606\n",
       "std      14276.826744\n",
       "min       1312.592463\n",
       "25%       5532.932779\n",
       "50%      11015.456400\n",
       "75%      19401.102168\n",
       "max      74611.400772\n",
       "Name: charges, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer here\n",
    "dataframe['charges'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/55650457/3187537\n",
    "def make_labels(ax, boxplot):\n",
    "\n",
    "    # Grab the relevant Line2D instances from the boxplot dictionary\n",
    "    iqr = boxplot['boxes'][0]\n",
    "    caps = boxplot['caps']\n",
    "    med = boxplot['medians'][0]\n",
    "    fly = boxplot['fliers'][0]\n",
    "    avg = boxplot['means'][0]\n",
    "    # The x position of the median line\n",
    "    xpos = med.get_xdata()\n",
    "    \n",
    "    # Lets make the text have a horizontal offset which is some \n",
    "    # fraction of the width of the box\n",
    "    xoff = 0.10 * (xpos[1] - xpos[0])\n",
    "\n",
    "    # The x position of the labels\n",
    "    xlabel = xpos[1] + xoff\n",
    "\n",
    "    # The median is the y-position of the median line\n",
    "    #print(med)\n",
    "    median = med.get_ydata()[1]\n",
    "    #print(median)\n",
    "    \n",
    "    # The mean \n",
    "    mean = avg.get_ydata()[1]\n",
    "   \n",
    "    # The 25th and 75th percentiles are found from the\n",
    "    # top and bottom (max and min) of the box\n",
    "    pc25 = iqr.get_ydata().min()\n",
    "    pc75 = iqr.get_ydata().max()\n",
    "\n",
    "    # The caps give the vertical position of the ends of the whiskers\n",
    "    capbottom = caps[0].get_ydata()[0]\n",
    "    captop = caps[1].get_ydata()[0]\n",
    "\n",
    "    # Make some labels on the figure using the values derived above\n",
    "    ax.text(xlabel, median,\n",
    "            'Median = {:.2f}'.format(median), va='center')\n",
    "    ax.text(xlabel, pc25,\n",
    "            '25th percentile = {:.2f}'.format(pc25), va='center')\n",
    "    ax.text(xlabel, pc75,\n",
    "            '75th percentile = {:.2f}'.format(pc75), va='center')\n",
    "    ax.text(xlabel, capbottom,\n",
    "            'Min. value = {:.2f}'.format(capbottom), va='center')\n",
    "    ax.text(xlabel, captop,\n",
    "            'Max. value = {:.2f}'.format(captop), va='center')\n",
    "    ax.text(xlabel, mean,\n",
    "             'Avg. value = {:.2f}'.format(mean), va='center')\n",
    "\n",
    "    \n",
    "# Make the figure\n",
    "red_diamond = dict(markerfacecolor='r', marker='D')\n",
    "fig3, ax3 = plt.subplots(figsize=(16, 10))\n",
    "ax3.set_title('Distribution of charges')\n",
    "\n",
    "# Create the boxplot and store the resulting python dictionary\n",
    "my_boxes = ax3.boxplot(dataframe['charges'], flierprops=red_diamond, showmeans=True, meanline = True)\n",
    "\n",
    "# Call the function to make labels\n",
    "make_labels(ax3, my_boxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to commit your notebook to Jovian after every step, so that you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[jovian] Error: Failed to detect notebook filename. Please provide the correct notebook filename as the \"filename\" argument to \"jovian.commit\".\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the dataset for training\n",
    "\n",
    "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    \n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    \n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    \n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[23.     ,  0.     , 33.81905,  0.     ,  0.     ,  0.     ],\n",
       "        [20.     ,  1.     , 21.34   ,  1.     ,  0.     ,  3.     ],\n",
       "        [28.     ,  0.     , 28.0136 ,  1.     ,  0.     ,  0.     ],\n",
       "        ...,\n",
       "        [53.     ,  0.     , 25.899  ,  2.     ,  0.     ,  3.     ],\n",
       "        [26.     ,  1.     , 26.2482 ,  0.     ,  1.     ,  2.     ],\n",
       "        [19.     ,  0.     , 27.4607 ,  0.     ,  1.     ,  1.     ]]),\n",
       " array([[ 3392.4025395],\n",
       "        [ 2298.7926   ],\n",
       "        [ 5075.150184 ],\n",
       "        ...,\n",
       "        [13046.4126   ],\n",
       "        [19940.709438 ],\n",
       "        [20438.711163 ]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs_array).float()\n",
    "targets = torch.from_numpy(targets_array).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype, targets.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.1 # between 0.1 and 0.2\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create data loaders for training & validation.\n",
    "\n",
    "**Q: Pick a batch size for the data loader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a batch of data to verify everything is working fine so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[39.0000,  0.0000, 33.0770,  3.0000,  0.0000,  3.0000],\n",
      "        [28.0000,  0.0000, 16.7713,  0.0000,  0.0000,  0.0000],\n",
      "        [34.0000,  1.0000, 21.7474,  2.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [29.0000,  0.0000, 30.2252,  0.0000,  0.0000,  0.0000],\n",
      "        [28.0000,  1.0000, 32.8054,  0.0000,  0.0000,  1.0000],\n",
      "        [47.0000,  1.0000, 34.9976,  1.0000,  1.0000,  2.0000]])\n",
      "targets: tensor([[ 8679.6709],\n",
      "        [ 4367.1714],\n",
      "        [32029.8086],\n",
      "        [43856.8047],\n",
      "        [10067.2461],\n",
      "        [ 8920.8174],\n",
      "        [ 4954.8647],\n",
      "        [ 9040.8867],\n",
      "        [23072.0430],\n",
      "        [ 3953.3247],\n",
      "        [10041.2939],\n",
      "        [29697.2871],\n",
      "        [13205.2500],\n",
      "        [ 5187.0635],\n",
      "        [ 2003.3055],\n",
      "        [19989.7637],\n",
      "        [ 2200.1697],\n",
      "        [11750.6016],\n",
      "        [ 3535.5166],\n",
      "        [ 7338.7983],\n",
      "        [11172.9912],\n",
      "        [28706.7676],\n",
      "        [ 1905.3109],\n",
      "        [27248.0469],\n",
      "        [13977.5527],\n",
      "        [50290.6445],\n",
      "        [10649.1035],\n",
      "        [17190.4238],\n",
      "        [ 4371.6636],\n",
      "        [22177.6074],\n",
      "        [ 9507.1680],\n",
      "        [39974.5391],\n",
      "        [40333.2227],\n",
      "        [43834.4531],\n",
      "        [17071.0391],\n",
      "        [43099.8359],\n",
      "        [ 6610.1665],\n",
      "        [ 7116.4141],\n",
      "        [ 4139.1123],\n",
      "        [11554.5459],\n",
      "        [ 9505.5576],\n",
      "        [11865.1289],\n",
      "        [24428.1777],\n",
      "        [ 5996.5024],\n",
      "        [12395.2441],\n",
      "        [ 4727.4531],\n",
      "        [ 6298.7373],\n",
      "        [ 2134.4138],\n",
      "        [ 2802.3508],\n",
      "        [55531.5859],\n",
      "        [15081.3672],\n",
      "        [13499.9521],\n",
      "        [12843.3311],\n",
      "        [ 6421.2666],\n",
      "        [ 2575.2837],\n",
      "        [52066.6367],\n",
      "        [40077.2422],\n",
      "        [34542.1055],\n",
      "        [ 1918.2888],\n",
      "        [22463.9336],\n",
      "        [ 2371.5598],\n",
      "        [ 2893.8008],\n",
      "        [49285.6836],\n",
      "        [ 3039.4014],\n",
      "        [ 2752.9731],\n",
      "        [41137.3594],\n",
      "        [ 5592.1348],\n",
      "        [16869.3887],\n",
      "        [30411.6016],\n",
      "        [ 9751.4697],\n",
      "        [14163.2246],\n",
      "        [25362.4219],\n",
      "        [14430.5791],\n",
      "        [ 9632.7236],\n",
      "        [39671.8320],\n",
      "        [ 6943.2246],\n",
      "        [29855.0234],\n",
      "        [14786.6885],\n",
      "        [ 5211.6509],\n",
      "        [11100.5439],\n",
      "        [ 4618.4731],\n",
      "        [ 6828.7290],\n",
      "        [39663.7656],\n",
      "        [17557.6973],\n",
      "        [57124.6055],\n",
      "        [14601.2598],\n",
      "        [12781.0020],\n",
      "        [ 5287.0269],\n",
      "        [14350.6436],\n",
      "        [ 4957.4380],\n",
      "        [32878.5586],\n",
      "        [15259.0479],\n",
      "        [14796.0625],\n",
      "        [ 5759.8115],\n",
      "        [ 5181.7988],\n",
      "        [ 2363.6072],\n",
      "        [16350.1133],\n",
      "        [ 4073.7856],\n",
      "        [16679.7461],\n",
      "        [ 7850.9243],\n",
      "        [16753.2656],\n",
      "        [44946.1055],\n",
      "        [12209.0264],\n",
      "        [ 7249.8442],\n",
      "        [ 8923.5820],\n",
      "        [ 9723.6816],\n",
      "        [ 9379.9619],\n",
      "        [ 2877.8726],\n",
      "        [ 2149.6199],\n",
      "        [ 8680.7891],\n",
      "        [10337.2598],\n",
      "        [ 8568.7695],\n",
      "        [44747.3438],\n",
      "        [12080.4912],\n",
      "        [ 8243.2334],\n",
      "        [ 3146.7097],\n",
      "        [55305.7305],\n",
      "        [11964.7529],\n",
      "        [ 5316.7549],\n",
      "        [ 5299.4883],\n",
      "        [21363.2832],\n",
      "        [ 7555.6768],\n",
      "        [ 2018.8962],\n",
      "        [11267.5645],\n",
      "        [ 1913.8083],\n",
      "        [ 1909.0519],\n",
      "        [19651.8027],\n",
      "        [ 4161.5991],\n",
      "        [16519.9551],\n",
      "        [15751.1484],\n",
      "        [ 3397.7830],\n",
      "        [ 6011.7603],\n",
      "        [ 5659.9712],\n",
      "        [12802.2949],\n",
      "        [31728.0527],\n",
      "        [21771.6660],\n",
      "        [27268.6602],\n",
      "        [11053.2832],\n",
      "        [12626.5928],\n",
      "        [14776.9795],\n",
      "        [16085.0107],\n",
      "        [42341.2500],\n",
      "        [ 8024.4209],\n",
      "        [16655.0352],\n",
      "        [ 8931.4531],\n",
      "        [ 1772.9535],\n",
      "        [ 2021.2218],\n",
      "        [19211.9785],\n",
      "        [ 4524.2207],\n",
      "        [13495.8008],\n",
      "        [ 5189.0601],\n",
      "        [ 8120.8647],\n",
      "        [ 8593.7480],\n",
      "        [ 4264.7544],\n",
      "        [ 5187.6816],\n",
      "        [47939.7734],\n",
      "        [ 3721.1968],\n",
      "        [ 3389.8684],\n",
      "        [11467.0391],\n",
      "        [ 6131.6948],\n",
      "        [ 2727.8772],\n",
      "        [22257.2148],\n",
      "        [14150.5391],\n",
      "        [40502.8750],\n",
      "        [16939.6465],\n",
      "        [35174.3906],\n",
      "        [15224.2842],\n",
      "        [22356.1016],\n",
      "        [44790.8164],\n",
      "        [ 4334.0947],\n",
      "        [ 2032.7300],\n",
      "        [ 8047.2368],\n",
      "        [ 1452.6311],\n",
      "        [ 2149.5674],\n",
      "        [ 9632.1875],\n",
      "        [12955.6162],\n",
      "        [49495.3203],\n",
      "        [ 1959.3198],\n",
      "        [34317.2500],\n",
      "        [13975.1758],\n",
      "        [40294.6328],\n",
      "        [ 2001.9014],\n",
      "        [ 6990.0303],\n",
      "        [ 1995.5807],\n",
      "        [10886.5010],\n",
      "        [ 3834.2783],\n",
      "        [12786.7529],\n",
      "        [13274.2568],\n",
      "        [ 9344.1758],\n",
      "        [24314.3418],\n",
      "        [ 1910.6620],\n",
      "        [20665.8789],\n",
      "        [16860.7910],\n",
      "        [ 5221.9502],\n",
      "        [12210.2324],\n",
      "        [26203.0215],\n",
      "        [10379.2471],\n",
      "        [ 6264.2676],\n",
      "        [ 8609.0654],\n",
      "        [ 9970.7432],\n",
      "        [21423.5684],\n",
      "        [12665.5469],\n",
      "        [51717.1055],\n",
      "        [ 5864.5913],\n",
      "        [ 7151.4531],\n",
      "        [ 4517.6152],\n",
      "        [12909.3838],\n",
      "        [ 2523.3997],\n",
      "        [ 5576.2456],\n",
      "        [38969.8359],\n",
      "        [ 8796.0898],\n",
      "        [14544.2158],\n",
      "        [15358.2129],\n",
      "        [ 1897.9606],\n",
      "        [14315.9316],\n",
      "        [ 9985.5664],\n",
      "        [ 5078.3159],\n",
      "        [ 7722.2407],\n",
      "        [ 9989.7979],\n",
      "        [ 8919.5156],\n",
      "        [ 4163.5854],\n",
      "        [ 3169.6609],\n",
      "        [16799.2871],\n",
      "        [46328.9648],\n",
      "        [ 9691.9062],\n",
      "        [12402.6416],\n",
      "        [ 6095.2070],\n",
      "        [10294.3535],\n",
      "        [ 8366.7773],\n",
      "        [ 8173.0430],\n",
      "        [ 5436.7080],\n",
      "        [33096.8398],\n",
      "        [ 7157.4688],\n",
      "        [ 1998.3617],\n",
      "        [ 9606.9873],\n",
      "        [16058.8018],\n",
      "        [14513.7090],\n",
      "        [46345.7578],\n",
      "        [ 1329.0507],\n",
      "        [ 2124.5747],\n",
      "        [13733.2832],\n",
      "        [10981.8086],\n",
      "        [50142.5000],\n",
      "        [41122.6094],\n",
      "        [ 2578.3711],\n",
      "        [ 8248.5254],\n",
      "        [10449.1934],\n",
      "        [16789.3301],\n",
      "        [13334.3730],\n",
      "        [10790.2109],\n",
      "        [11689.5137],\n",
      "        [ 2902.7456],\n",
      "        [ 8379.5547],\n",
      "        [ 4614.0068],\n",
      "        [23017.8027],\n",
      "        [49387.0312]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(\"inputs:\", xb)\n",
    "    print(\"targets:\", yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work by committing to Jovian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[jovian] Error: Failed to detect notebook filename. Please provide the correct notebook filename as the \"filename\" argument to \"jovian.commit\".\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Linear Regression Model\n",
    "\n",
    "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n",
    "\n",
    "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear(xb)                          # fill this\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.l1_loss(out, targets)                          # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out, targets)                           # fill this    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the weights and biases of the model using `model.parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2158, -0.2330, -0.0527, -0.2446,  0.0716, -0.3923]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2563], requires_grad=True)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final commit before we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model to fit the data\n",
    "\n",
    "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 19247.181640625}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader) # Use the the evaluate function\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n",
    "\n",
    "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 19244.7246\n",
      "Epoch [40], val_loss: 19242.2637\n",
      "Epoch [60], val_loss: 19239.8027\n",
      "Epoch [80], val_loss: 19237.3438\n",
      "Epoch [100], val_loss: 19234.8848\n",
      "Epoch [120], val_loss: 19232.4238\n",
      "Epoch [140], val_loss: 19229.9707\n",
      "Epoch [160], val_loss: 19227.5078\n",
      "Epoch [180], val_loss: 19225.0430\n",
      "Epoch [200], val_loss: 19222.5820\n",
      "Epoch [220], val_loss: 19220.1211\n",
      "Epoch [240], val_loss: 19217.6562\n",
      "Epoch [260], val_loss: 19215.1973\n",
      "Epoch [280], val_loss: 19212.7383\n",
      "Epoch [300], val_loss: 19210.2773\n",
      "Epoch [320], val_loss: 19207.8223\n",
      "Epoch [340], val_loss: 19205.3613\n",
      "Epoch [360], val_loss: 19202.9004\n",
      "Epoch [380], val_loss: 19200.4375\n",
      "Epoch [400], val_loss: 19197.9824\n",
      "Epoch [420], val_loss: 19195.5137\n",
      "Epoch [440], val_loss: 19193.0566\n",
      "Epoch [460], val_loss: 19190.6016\n",
      "Epoch [480], val_loss: 19188.1367\n",
      "Epoch [500], val_loss: 19185.6719\n",
      "Epoch [520], val_loss: 19183.2109\n",
      "Epoch [540], val_loss: 19180.7480\n",
      "Epoch [560], val_loss: 19178.2852\n",
      "Epoch [580], val_loss: 19175.8262\n",
      "Epoch [600], val_loss: 19173.3691\n",
      "Epoch [620], val_loss: 19170.9062\n",
      "Epoch [640], val_loss: 19168.4512\n",
      "Epoch [660], val_loss: 19165.9883\n",
      "Epoch [680], val_loss: 19163.5234\n",
      "Epoch [700], val_loss: 19161.0723\n",
      "Epoch [720], val_loss: 19158.6055\n",
      "Epoch [740], val_loss: 19156.1465\n",
      "Epoch [760], val_loss: 19153.6914\n",
      "Epoch [780], val_loss: 19151.2285\n",
      "Epoch [800], val_loss: 19148.7656\n",
      "Epoch [820], val_loss: 19146.3086\n",
      "Epoch [840], val_loss: 19143.8438\n",
      "Epoch [860], val_loss: 19141.3848\n",
      "Epoch [880], val_loss: 19138.9297\n",
      "Epoch [900], val_loss: 19136.4629\n",
      "Epoch [920], val_loss: 19134.0020\n",
      "Epoch [940], val_loss: 19131.5430\n",
      "Epoch [960], val_loss: 19129.0801\n",
      "Epoch [980], val_loss: 19126.6289\n",
      "Epoch [1000], val_loss: 19124.1641\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 10 ** -5\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 19099.5566\n",
      "Epoch [40], val_loss: 19074.9375\n",
      "Epoch [60], val_loss: 19050.3262\n",
      "Epoch [80], val_loss: 19025.7188\n",
      "Epoch [100], val_loss: 19001.1074\n",
      "Epoch [120], val_loss: 18976.4844\n",
      "Epoch [140], val_loss: 18951.8789\n",
      "Epoch [160], val_loss: 18927.2559\n",
      "Epoch [180], val_loss: 18902.6582\n",
      "Epoch [200], val_loss: 18878.0527\n",
      "Epoch [220], val_loss: 18853.4434\n",
      "Epoch [240], val_loss: 18828.8379\n",
      "Epoch [260], val_loss: 18804.2285\n",
      "Epoch [280], val_loss: 18779.5918\n",
      "Epoch [300], val_loss: 18755.0020\n",
      "Epoch [320], val_loss: 18730.3867\n",
      "Epoch [340], val_loss: 18705.7988\n",
      "Epoch [360], val_loss: 18681.1914\n",
      "Epoch [380], val_loss: 18656.5781\n",
      "Epoch [400], val_loss: 18631.9707\n",
      "Epoch [420], val_loss: 18607.3496\n",
      "Epoch [440], val_loss: 18582.7598\n",
      "Epoch [460], val_loss: 18558.1504\n",
      "Epoch [480], val_loss: 18533.5410\n",
      "Epoch [500], val_loss: 18508.9199\n",
      "Epoch [520], val_loss: 18484.3125\n",
      "Epoch [540], val_loss: 18459.7324\n",
      "Epoch [560], val_loss: 18435.1387\n",
      "Epoch [580], val_loss: 18410.5273\n",
      "Epoch [600], val_loss: 18385.9043\n",
      "Epoch [620], val_loss: 18361.3008\n",
      "Epoch [640], val_loss: 18336.6914\n",
      "Epoch [660], val_loss: 18312.0820\n",
      "Epoch [680], val_loss: 18287.4980\n",
      "Epoch [700], val_loss: 18262.8887\n",
      "Epoch [720], val_loss: 18238.2910\n",
      "Epoch [740], val_loss: 18213.6934\n",
      "Epoch [760], val_loss: 18189.0898\n",
      "Epoch [780], val_loss: 18164.5039\n",
      "Epoch [800], val_loss: 18139.9023\n",
      "Epoch [820], val_loss: 18115.2930\n",
      "Epoch [840], val_loss: 18090.6953\n",
      "Epoch [860], val_loss: 18066.0918\n",
      "Epoch [880], val_loss: 18041.4824\n",
      "Epoch [900], val_loss: 18016.8691\n",
      "Epoch [920], val_loss: 17992.2734\n",
      "Epoch [940], val_loss: 17967.6797\n",
      "Epoch [960], val_loss: 17943.0645\n",
      "Epoch [980], val_loss: 17918.4707\n",
      "Epoch [1000], val_loss: 17893.8574\n",
      "Epoch [1020], val_loss: 17869.2539\n",
      "Epoch [1040], val_loss: 17844.6543\n",
      "Epoch [1060], val_loss: 17820.0371\n",
      "Epoch [1080], val_loss: 17795.4531\n",
      "Epoch [1100], val_loss: 17770.8516\n",
      "Epoch [1120], val_loss: 17746.2324\n",
      "Epoch [1140], val_loss: 17721.6562\n",
      "Epoch [1160], val_loss: 17697.0996\n",
      "Epoch [1180], val_loss: 17672.5254\n",
      "Epoch [1200], val_loss: 17647.9785\n",
      "Epoch [1220], val_loss: 17623.4336\n",
      "Epoch [1240], val_loss: 17598.8789\n",
      "Epoch [1260], val_loss: 17574.3086\n",
      "Epoch [1280], val_loss: 17549.7344\n",
      "Epoch [1300], val_loss: 17525.4844\n",
      "Epoch [1320], val_loss: 17501.2246\n",
      "Epoch [1340], val_loss: 17476.9883\n",
      "Epoch [1360], val_loss: 17452.7676\n",
      "Epoch [1380], val_loss: 17428.5469\n",
      "Epoch [1400], val_loss: 17404.3613\n",
      "Epoch [1420], val_loss: 17380.4199\n",
      "Epoch [1440], val_loss: 17356.4688\n",
      "Epoch [1460], val_loss: 17332.5215\n",
      "Epoch [1480], val_loss: 17308.6035\n",
      "Epoch [1500], val_loss: 17284.7402\n",
      "Epoch [1520], val_loss: 17260.9141\n",
      "Epoch [1540], val_loss: 17237.1055\n",
      "Epoch [1560], val_loss: 17213.2500\n",
      "Epoch [1580], val_loss: 17189.4355\n",
      "Epoch [1600], val_loss: 17165.9023\n",
      "Epoch [1620], val_loss: 17142.3867\n",
      "Epoch [1640], val_loss: 17118.8906\n",
      "Epoch [1660], val_loss: 17095.3926\n",
      "Epoch [1680], val_loss: 17071.8926\n",
      "Epoch [1700], val_loss: 17048.4082\n",
      "Epoch [1720], val_loss: 17024.9512\n",
      "Epoch [1740], val_loss: 17001.4785\n",
      "Epoch [1760], val_loss: 16977.9922\n",
      "Epoch [1780], val_loss: 16954.5664\n",
      "Epoch [1800], val_loss: 16931.1426\n",
      "Epoch [1820], val_loss: 16907.7832\n",
      "Epoch [1840], val_loss: 16884.4453\n",
      "Epoch [1860], val_loss: 16861.1328\n",
      "Epoch [1880], val_loss: 16837.8398\n",
      "Epoch [1900], val_loss: 16814.5508\n",
      "Epoch [1920], val_loss: 16791.3008\n",
      "Epoch [1940], val_loss: 16768.0312\n",
      "Epoch [1960], val_loss: 16744.8203\n",
      "Epoch [1980], val_loss: 16721.6543\n",
      "Epoch [2000], val_loss: 16698.4922\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "lr = 10 ** -4\n",
    "history2 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 16468.6465\n",
      "Epoch [40], val_loss: 16241.9219\n",
      "Epoch [60], val_loss: 16022.4355\n",
      "Epoch [80], val_loss: 15814.1201\n",
      "Epoch [100], val_loss: 15613.3252\n",
      "Epoch [120], val_loss: 15417.9053\n",
      "Epoch [140], val_loss: 15226.0430\n",
      "Epoch [160], val_loss: 15037.9766\n",
      "Epoch [180], val_loss: 14860.4453\n",
      "Epoch [200], val_loss: 14689.9717\n",
      "Epoch [220], val_loss: 14524.7666\n",
      "Epoch [240], val_loss: 14366.5762\n",
      "Epoch [260], val_loss: 14219.1260\n",
      "Epoch [280], val_loss: 14079.6309\n",
      "Epoch [300], val_loss: 13943.8662\n",
      "Epoch [320], val_loss: 13816.3066\n",
      "Epoch [340], val_loss: 13696.3340\n",
      "Epoch [360], val_loss: 13581.1562\n",
      "Epoch [380], val_loss: 13473.2988\n",
      "Epoch [400], val_loss: 13371.4902\n",
      "Epoch [420], val_loss: 13275.4424\n",
      "Epoch [440], val_loss: 13182.0439\n",
      "Epoch [460], val_loss: 13091.6904\n",
      "Epoch [480], val_loss: 13005.2627\n",
      "Epoch [500], val_loss: 12925.7393\n",
      "Epoch [520], val_loss: 12849.2939\n",
      "Epoch [540], val_loss: 12777.0859\n",
      "Epoch [560], val_loss: 12707.6934\n",
      "Epoch [580], val_loss: 12640.9326\n",
      "Epoch [600], val_loss: 12577.1826\n",
      "Epoch [620], val_loss: 12515.7139\n",
      "Epoch [640], val_loss: 12456.8770\n",
      "Epoch [660], val_loss: 12401.5420\n",
      "Epoch [680], val_loss: 12349.3525\n",
      "Epoch [700], val_loss: 12299.4277\n",
      "Epoch [720], val_loss: 12252.6416\n",
      "Epoch [740], val_loss: 12209.1270\n",
      "Epoch [760], val_loss: 12167.7988\n",
      "Epoch [780], val_loss: 12129.3828\n",
      "Epoch [800], val_loss: 12094.3516\n",
      "Epoch [820], val_loss: 12061.8623\n",
      "Epoch [840], val_loss: 12031.5439\n",
      "Epoch [860], val_loss: 12003.2295\n",
      "Epoch [880], val_loss: 11976.8281\n",
      "Epoch [900], val_loss: 11951.1348\n",
      "Epoch [920], val_loss: 11928.4795\n",
      "Epoch [940], val_loss: 11907.1982\n",
      "Epoch [960], val_loss: 11887.4521\n",
      "Epoch [980], val_loss: 11868.4453\n",
      "Epoch [1000], val_loss: 11850.4531\n",
      "Epoch [1020], val_loss: 11833.1572\n",
      "Epoch [1040], val_loss: 11817.4297\n",
      "Epoch [1060], val_loss: 11802.5537\n",
      "Epoch [1080], val_loss: 11788.7744\n",
      "Epoch [1100], val_loss: 11775.5879\n",
      "Epoch [1120], val_loss: 11762.7930\n",
      "Epoch [1140], val_loss: 11750.2588\n",
      "Epoch [1160], val_loss: 11738.8350\n",
      "Epoch [1180], val_loss: 11728.5137\n",
      "Epoch [1200], val_loss: 11719.6816\n",
      "Epoch [1220], val_loss: 11711.3604\n",
      "Epoch [1240], val_loss: 11703.6963\n",
      "Epoch [1260], val_loss: 11696.2246\n",
      "Epoch [1280], val_loss: 11689.2979\n",
      "Epoch [1300], val_loss: 11682.7686\n",
      "Epoch [1320], val_loss: 11676.1533\n",
      "Epoch [1340], val_loss: 11670.0391\n",
      "Epoch [1360], val_loss: 11664.5537\n",
      "Epoch [1380], val_loss: 11659.0527\n",
      "Epoch [1400], val_loss: 11654.0273\n",
      "Epoch [1420], val_loss: 11649.4072\n",
      "Epoch [1440], val_loss: 11644.7812\n",
      "Epoch [1460], val_loss: 11640.4355\n",
      "Epoch [1480], val_loss: 11636.1611\n",
      "Epoch [1500], val_loss: 11631.9844\n",
      "Epoch [1520], val_loss: 11628.0791\n",
      "Epoch [1540], val_loss: 11624.0986\n",
      "Epoch [1560], val_loss: 11620.2861\n",
      "Epoch [1580], val_loss: 11616.4893\n",
      "Epoch [1600], val_loss: 11612.5400\n",
      "Epoch [1620], val_loss: 11608.8711\n",
      "Epoch [1640], val_loss: 11605.5752\n",
      "Epoch [1660], val_loss: 11602.0938\n",
      "Epoch [1680], val_loss: 11598.9658\n",
      "Epoch [1700], val_loss: 11595.9443\n",
      "Epoch [1720], val_loss: 11593.1035\n",
      "Epoch [1740], val_loss: 11590.5605\n",
      "Epoch [1760], val_loss: 11587.8096\n",
      "Epoch [1780], val_loss: 11585.2109\n",
      "Epoch [1800], val_loss: 11582.6133\n",
      "Epoch [1820], val_loss: 11580.2754\n",
      "Epoch [1840], val_loss: 11577.8809\n",
      "Epoch [1860], val_loss: 11575.8213\n",
      "Epoch [1880], val_loss: 11573.6602\n",
      "Epoch [1900], val_loss: 11571.6973\n",
      "Epoch [1920], val_loss: 11569.7461\n",
      "Epoch [1940], val_loss: 11567.9014\n",
      "Epoch [1960], val_loss: 11566.0801\n",
      "Epoch [1980], val_loss: 11564.3193\n",
      "Epoch [2000], val_loss: 11562.5518\n",
      "Epoch [2020], val_loss: 11560.6885\n",
      "Epoch [2040], val_loss: 11558.7295\n",
      "Epoch [2060], val_loss: 11557.1143\n",
      "Epoch [2080], val_loss: 11555.4248\n",
      "Epoch [2100], val_loss: 11553.6689\n",
      "Epoch [2120], val_loss: 11552.2451\n",
      "Epoch [2140], val_loss: 11550.8135\n",
      "Epoch [2160], val_loss: 11549.5684\n",
      "Epoch [2180], val_loss: 11548.2939\n",
      "Epoch [2200], val_loss: 11547.0410\n",
      "Epoch [2220], val_loss: 11545.7168\n",
      "Epoch [2240], val_loss: 11544.5576\n",
      "Epoch [2260], val_loss: 11543.2637\n",
      "Epoch [2280], val_loss: 11542.0264\n",
      "Epoch [2300], val_loss: 11540.7607\n",
      "Epoch [2320], val_loss: 11539.6943\n",
      "Epoch [2340], val_loss: 11538.4746\n",
      "Epoch [2360], val_loss: 11537.3750\n",
      "Epoch [2380], val_loss: 11536.3193\n",
      "Epoch [2400], val_loss: 11535.1445\n",
      "Epoch [2420], val_loss: 11534.0850\n",
      "Epoch [2440], val_loss: 11532.9111\n",
      "Epoch [2460], val_loss: 11531.8789\n",
      "Epoch [2480], val_loss: 11531.0537\n",
      "Epoch [2500], val_loss: 11530.1514\n",
      "Epoch [2520], val_loss: 11529.2451\n",
      "Epoch [2540], val_loss: 11528.3428\n",
      "Epoch [2560], val_loss: 11527.8213\n",
      "Epoch [2580], val_loss: 11526.9180\n",
      "Epoch [2600], val_loss: 11526.1221\n",
      "Epoch [2620], val_loss: 11525.3154\n",
      "Epoch [2640], val_loss: 11524.3193\n",
      "Epoch [2660], val_loss: 11523.3994\n",
      "Epoch [2680], val_loss: 11522.4492\n",
      "Epoch [2700], val_loss: 11521.4873\n",
      "Epoch [2720], val_loss: 11520.5371\n",
      "Epoch [2740], val_loss: 11519.5596\n",
      "Epoch [2760], val_loss: 11518.5088\n",
      "Epoch [2780], val_loss: 11517.7119\n",
      "Epoch [2800], val_loss: 11516.7158\n",
      "Epoch [2820], val_loss: 11515.9980\n",
      "Epoch [2840], val_loss: 11515.2324\n",
      "Epoch [2860], val_loss: 11514.3555\n",
      "Epoch [2880], val_loss: 11513.5117\n",
      "Epoch [2900], val_loss: 11512.5361\n",
      "Epoch [2920], val_loss: 11511.7939\n",
      "Epoch [2940], val_loss: 11510.9258\n",
      "Epoch [2960], val_loss: 11510.2031\n",
      "Epoch [2980], val_loss: 11509.3018\n",
      "Epoch [3000], val_loss: 11508.4033\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "lr = 10 ** -3\n",
    "history3 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 11500.2891\n",
      "Epoch [40], val_loss: 11492.2773\n",
      "Epoch [60], val_loss: 11482.8877\n",
      "Epoch [80], val_loss: 11474.0264\n",
      "Epoch [100], val_loss: 11463.5713\n",
      "Epoch [120], val_loss: 11456.0156\n",
      "Epoch [140], val_loss: 11448.5889\n",
      "Epoch [160], val_loss: 11440.5742\n",
      "Epoch [180], val_loss: 11433.7256\n",
      "Epoch [200], val_loss: 11427.8984\n",
      "Epoch [220], val_loss: 11421.3779\n",
      "Epoch [240], val_loss: 11413.8252\n",
      "Epoch [260], val_loss: 11406.8955\n",
      "Epoch [280], val_loss: 11400.0156\n",
      "Epoch [300], val_loss: 11393.3887\n",
      "Epoch [320], val_loss: 11383.5146\n",
      "Epoch [340], val_loss: 11375.8154\n",
      "Epoch [360], val_loss: 11368.4619\n",
      "Epoch [380], val_loss: 11361.5898\n",
      "Epoch [400], val_loss: 11355.5869\n",
      "Epoch [420], val_loss: 11349.2607\n",
      "Epoch [440], val_loss: 11342.1445\n",
      "Epoch [460], val_loss: 11333.4980\n",
      "Epoch [480], val_loss: 11326.9834\n",
      "Epoch [500], val_loss: 11317.7852\n",
      "Epoch [520], val_loss: 11313.0020\n",
      "Epoch [540], val_loss: 11306.7871\n",
      "Epoch [560], val_loss: 11300.7354\n",
      "Epoch [580], val_loss: 11292.1094\n",
      "Epoch [600], val_loss: 11281.9785\n",
      "Epoch [620], val_loss: 11274.2568\n",
      "Epoch [640], val_loss: 11267.3770\n",
      "Epoch [660], val_loss: 11259.0977\n",
      "Epoch [680], val_loss: 11253.4951\n",
      "Epoch [700], val_loss: 11246.7627\n",
      "Epoch [720], val_loss: 11241.9014\n",
      "Epoch [740], val_loss: 11237.3711\n",
      "Epoch [760], val_loss: 11232.3613\n",
      "Epoch [780], val_loss: 11229.0576\n",
      "Epoch [800], val_loss: 11225.7852\n",
      "Epoch [820], val_loss: 11222.2061\n",
      "Epoch [840], val_loss: 11219.0830\n",
      "Epoch [860], val_loss: 11216.1406\n",
      "Epoch [880], val_loss: 11211.3652\n",
      "Epoch [900], val_loss: 11208.1445\n",
      "Epoch [920], val_loss: 11203.8086\n",
      "Epoch [940], val_loss: 11203.5186\n",
      "Epoch [960], val_loss: 11201.6045\n",
      "Epoch [980], val_loss: 11199.0479\n",
      "Epoch [1000], val_loss: 11196.5078\n",
      "Epoch [1020], val_loss: 11195.5869\n",
      "Epoch [1040], val_loss: 11191.9326\n",
      "Epoch [1060], val_loss: 11192.5205\n",
      "Epoch [1080], val_loss: 11188.6484\n",
      "Epoch [1100], val_loss: 11187.5615\n",
      "Epoch [1120], val_loss: 11186.7217\n",
      "Epoch [1140], val_loss: 11185.2246\n",
      "Epoch [1160], val_loss: 11182.9775\n",
      "Epoch [1180], val_loss: 11181.2295\n",
      "Epoch [1200], val_loss: 11178.9277\n",
      "Epoch [1220], val_loss: 11176.6289\n",
      "Epoch [1240], val_loss: 11175.8789\n",
      "Epoch [1260], val_loss: 11174.5898\n",
      "Epoch [1280], val_loss: 11176.0117\n",
      "Epoch [1300], val_loss: 11175.8408\n",
      "Epoch [1320], val_loss: 11176.1787\n",
      "Epoch [1340], val_loss: 11176.5283\n",
      "Epoch [1360], val_loss: 11174.5166\n",
      "Epoch [1380], val_loss: 11173.0391\n",
      "Epoch [1400], val_loss: 11172.3311\n",
      "Epoch [1420], val_loss: 11171.5156\n",
      "Epoch [1440], val_loss: 11170.7891\n",
      "Epoch [1460], val_loss: 11170.5674\n",
      "Epoch [1480], val_loss: 11168.4365\n",
      "Epoch [1500], val_loss: 11168.2080\n",
      "Epoch [1520], val_loss: 11167.5811\n",
      "Epoch [1540], val_loss: 11166.0469\n",
      "Epoch [1560], val_loss: 11166.0693\n",
      "Epoch [1580], val_loss: 11164.6172\n",
      "Epoch [1600], val_loss: 11162.8418\n",
      "Epoch [1620], val_loss: 11162.3223\n",
      "Epoch [1640], val_loss: 11163.0400\n",
      "Epoch [1660], val_loss: 11161.9248\n",
      "Epoch [1680], val_loss: 11161.4258\n",
      "Epoch [1700], val_loss: 11160.4404\n",
      "Epoch [1720], val_loss: 11159.2178\n",
      "Epoch [1740], val_loss: 11159.0996\n",
      "Epoch [1760], val_loss: 11158.3896\n",
      "Epoch [1780], val_loss: 11159.3096\n",
      "Epoch [1800], val_loss: 11158.4121\n",
      "Epoch [1820], val_loss: 11158.2070\n",
      "Epoch [1840], val_loss: 11156.9434\n",
      "Epoch [1860], val_loss: 11157.6123\n",
      "Epoch [1880], val_loss: 11156.8184\n",
      "Epoch [1900], val_loss: 11156.4570\n",
      "Epoch [1920], val_loss: 11156.9326\n",
      "Epoch [1940], val_loss: 11156.3779\n",
      "Epoch [1960], val_loss: 11156.9316\n",
      "Epoch [1980], val_loss: 11156.8574\n",
      "Epoch [2000], val_loss: 11155.9121\n",
      "Epoch [2020], val_loss: 11155.9463\n",
      "Epoch [2040], val_loss: 11155.6562\n",
      "Epoch [2060], val_loss: 11154.2012\n",
      "Epoch [2080], val_loss: 11153.7314\n",
      "Epoch [2100], val_loss: 11154.0801\n",
      "Epoch [2120], val_loss: 11153.1133\n",
      "Epoch [2140], val_loss: 11153.5479\n",
      "Epoch [2160], val_loss: 11153.4492\n",
      "Epoch [2180], val_loss: 11152.9668\n",
      "Epoch [2200], val_loss: 11153.0225\n",
      "Epoch [2220], val_loss: 11152.7256\n",
      "Epoch [2240], val_loss: 11151.9072\n",
      "Epoch [2260], val_loss: 11152.0000\n",
      "Epoch [2280], val_loss: 11151.9863\n",
      "Epoch [2300], val_loss: 11151.0146\n",
      "Epoch [2320], val_loss: 11151.0752\n",
      "Epoch [2340], val_loss: 11150.3086\n",
      "Epoch [2360], val_loss: 11150.5830\n",
      "Epoch [2380], val_loss: 11150.3926\n",
      "Epoch [2400], val_loss: 11150.1318\n",
      "Epoch [2420], val_loss: 11150.1104\n",
      "Epoch [2440], val_loss: 11149.3457\n",
      "Epoch [2460], val_loss: 11148.8652\n",
      "Epoch [2480], val_loss: 11149.5176\n",
      "Epoch [2500], val_loss: 11148.8223\n",
      "Epoch [2520], val_loss: 11149.5127\n",
      "Epoch [2540], val_loss: 11148.5850\n",
      "Epoch [2560], val_loss: 11149.2119\n",
      "Epoch [2580], val_loss: 11149.0098\n",
      "Epoch [2600], val_loss: 11148.8516\n",
      "Epoch [2620], val_loss: 11148.9941\n",
      "Epoch [2640], val_loss: 11147.8506\n",
      "Epoch [2660], val_loss: 11148.3721\n",
      "Epoch [2680], val_loss: 11148.1016\n",
      "Epoch [2700], val_loss: 11148.2920\n",
      "Epoch [2720], val_loss: 11148.0459\n",
      "Epoch [2740], val_loss: 11148.2666\n",
      "Epoch [2760], val_loss: 11148.1963\n",
      "Epoch [2780], val_loss: 11147.2422\n",
      "Epoch [2800], val_loss: 11146.3008\n",
      "Epoch [2820], val_loss: 11147.7549\n",
      "Epoch [2840], val_loss: 11147.5576\n",
      "Epoch [2860], val_loss: 11147.0781\n",
      "Epoch [2880], val_loss: 11147.0576\n",
      "Epoch [2900], val_loss: 11145.9814\n",
      "Epoch [2920], val_loss: 11146.2646\n",
      "Epoch [2940], val_loss: 11145.4922\n",
      "Epoch [2960], val_loss: 11145.7061\n",
      "Epoch [2980], val_loss: 11145.4600\n",
      "Epoch [3000], val_loss: 11145.3945\n",
      "Epoch [3020], val_loss: 11145.4385\n",
      "Epoch [3040], val_loss: 11144.7764\n",
      "Epoch [3060], val_loss: 11145.8008\n",
      "Epoch [3080], val_loss: 11145.7988\n",
      "Epoch [3100], val_loss: 11145.1992\n",
      "Epoch [3120], val_loss: 11144.9404\n",
      "Epoch [3140], val_loss: 11144.4824\n",
      "Epoch [3160], val_loss: 11144.3574\n",
      "Epoch [3180], val_loss: 11143.4746\n",
      "Epoch [3200], val_loss: 11144.3867\n",
      "Epoch [3220], val_loss: 11145.2324\n",
      "Epoch [3240], val_loss: 11143.5898\n",
      "Epoch [3260], val_loss: 11143.4482\n",
      "Epoch [3280], val_loss: 11143.2871\n",
      "Epoch [3300], val_loss: 11143.3066\n",
      "Epoch [3320], val_loss: 11143.7568\n",
      "Epoch [3340], val_loss: 11143.0693\n",
      "Epoch [3360], val_loss: 11142.5977\n",
      "Epoch [3380], val_loss: 11143.6602\n",
      "Epoch [3400], val_loss: 11141.6299\n",
      "Epoch [3420], val_loss: 11141.2490\n",
      "Epoch [3440], val_loss: 11142.3584\n",
      "Epoch [3460], val_loss: 11142.6094\n",
      "Epoch [3480], val_loss: 11141.1338\n",
      "Epoch [3500], val_loss: 11141.5518\n"
     ]
    }
   ],
   "source": [
    "epochs = 3500\n",
    "lr = 10 ** -2\n",
    "history4 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 11134.8672\n",
      "Epoch [40], val_loss: 11136.5674\n",
      "Epoch [60], val_loss: 11136.0098\n",
      "Epoch [80], val_loss: 11130.1289\n",
      "Epoch [100], val_loss: 11131.7520\n",
      "Epoch [120], val_loss: 11130.1699\n",
      "Epoch [140], val_loss: 11132.7803\n",
      "Epoch [160], val_loss: 11127.2383\n",
      "Epoch [180], val_loss: 11128.7051\n",
      "Epoch [200], val_loss: 11121.6396\n",
      "Epoch [220], val_loss: 11123.1523\n",
      "Epoch [240], val_loss: 11120.6113\n",
      "Epoch [260], val_loss: 11120.0195\n",
      "Epoch [280], val_loss: 11125.9834\n",
      "Epoch [300], val_loss: 11114.0352\n",
      "Epoch [320], val_loss: 11117.2480\n",
      "Epoch [340], val_loss: 11114.1963\n",
      "Epoch [360], val_loss: 11114.4854\n",
      "Epoch [380], val_loss: 11108.4160\n",
      "Epoch [400], val_loss: 11111.4072\n",
      "Epoch [420], val_loss: 11105.9492\n",
      "Epoch [440], val_loss: 11108.7744\n",
      "Epoch [460], val_loss: 11111.7109\n",
      "Epoch [480], val_loss: 11099.3828\n",
      "Epoch [500], val_loss: 11099.7314\n",
      "Epoch [520], val_loss: 11103.1270\n",
      "Epoch [540], val_loss: 11096.3398\n",
      "Epoch [560], val_loss: 11102.4150\n",
      "Epoch [580], val_loss: 11100.3770\n",
      "Epoch [600], val_loss: 11100.0059\n",
      "Epoch [620], val_loss: 11094.5088\n",
      "Epoch [640], val_loss: 11091.9619\n",
      "Epoch [660], val_loss: 11092.6270\n",
      "Epoch [680], val_loss: 11083.5664\n",
      "Epoch [700], val_loss: 11090.8203\n",
      "Epoch [720], val_loss: 11089.1943\n",
      "Epoch [740], val_loss: 11087.1758\n",
      "Epoch [760], val_loss: 11085.7881\n",
      "Epoch [780], val_loss: 11082.6094\n",
      "Epoch [800], val_loss: 11081.1904\n",
      "Epoch [820], val_loss: 11082.9043\n",
      "Epoch [840], val_loss: 11081.7041\n",
      "Epoch [860], val_loss: 11076.3340\n",
      "Epoch [880], val_loss: 11081.0430\n",
      "Epoch [900], val_loss: 11074.4551\n",
      "Epoch [920], val_loss: 11071.9277\n",
      "Epoch [940], val_loss: 11071.1963\n",
      "Epoch [960], val_loss: 11067.3203\n",
      "Epoch [980], val_loss: 11067.6465\n",
      "Epoch [1000], val_loss: 11069.8721\n",
      "Epoch [1020], val_loss: 11067.1133\n",
      "Epoch [1040], val_loss: 11066.5508\n",
      "Epoch [1060], val_loss: 11062.8486\n",
      "Epoch [1080], val_loss: 11062.7080\n",
      "Epoch [1100], val_loss: 11065.2510\n",
      "Epoch [1120], val_loss: 11063.2930\n",
      "Epoch [1140], val_loss: 11062.3252\n",
      "Epoch [1160], val_loss: 11060.1172\n",
      "Epoch [1180], val_loss: 11061.5889\n",
      "Epoch [1200], val_loss: 11053.2217\n",
      "Epoch [1220], val_loss: 11058.2920\n",
      "Epoch [1240], val_loss: 11051.3926\n",
      "Epoch [1260], val_loss: 11052.5693\n",
      "Epoch [1280], val_loss: 11045.2979\n",
      "Epoch [1300], val_loss: 11049.0742\n",
      "Epoch [1320], val_loss: 11045.8750\n",
      "Epoch [1340], val_loss: 11044.0059\n",
      "Epoch [1360], val_loss: 11038.2549\n",
      "Epoch [1380], val_loss: 11039.6631\n",
      "Epoch [1400], val_loss: 11048.6025\n",
      "Epoch [1420], val_loss: 11041.0791\n",
      "Epoch [1440], val_loss: 11042.3857\n",
      "Epoch [1460], val_loss: 11035.3281\n",
      "Epoch [1480], val_loss: 11041.5615\n",
      "Epoch [1500], val_loss: 11037.3701\n",
      "Epoch [1520], val_loss: 11040.6221\n",
      "Epoch [1540], val_loss: 11038.4941\n",
      "Epoch [1560], val_loss: 11037.3340\n",
      "Epoch [1580], val_loss: 11033.8789\n",
      "Epoch [1600], val_loss: 11035.7061\n",
      "Epoch [1620], val_loss: 11038.0322\n",
      "Epoch [1640], val_loss: 11030.4434\n",
      "Epoch [1660], val_loss: 11027.4766\n",
      "Epoch [1680], val_loss: 11030.3955\n",
      "Epoch [1700], val_loss: 11026.6494\n",
      "Epoch [1720], val_loss: 11027.1865\n",
      "Epoch [1740], val_loss: 11026.3271\n",
      "Epoch [1760], val_loss: 11021.4648\n",
      "Epoch [1780], val_loss: 11026.4941\n",
      "Epoch [1800], val_loss: 11025.2822\n",
      "Epoch [1820], val_loss: 11028.7197\n",
      "Epoch [1840], val_loss: 11023.3457\n",
      "Epoch [1860], val_loss: 11024.7295\n",
      "Epoch [1880], val_loss: 11022.7070\n",
      "Epoch [1900], val_loss: 11025.4414\n",
      "Epoch [1920], val_loss: 11021.1475\n",
      "Epoch [1940], val_loss: 11019.9609\n",
      "Epoch [1960], val_loss: 11018.8682\n",
      "Epoch [1980], val_loss: 11016.0166\n",
      "Epoch [2000], val_loss: 11016.1895\n",
      "Epoch [2020], val_loss: 11010.9775\n",
      "Epoch [2040], val_loss: 11013.9219\n",
      "Epoch [2060], val_loss: 11008.1201\n",
      "Epoch [2080], val_loss: 11012.6553\n",
      "Epoch [2100], val_loss: 11014.6113\n",
      "Epoch [2120], val_loss: 11007.8867\n",
      "Epoch [2140], val_loss: 11007.8057\n",
      "Epoch [2160], val_loss: 11006.1738\n",
      "Epoch [2180], val_loss: 11007.6162\n",
      "Epoch [2200], val_loss: 11001.5645\n",
      "Epoch [2220], val_loss: 11002.5195\n",
      "Epoch [2240], val_loss: 11002.1455\n",
      "Epoch [2260], val_loss: 11000.9395\n",
      "Epoch [2280], val_loss: 11004.6816\n",
      "Epoch [2300], val_loss: 10998.6973\n",
      "Epoch [2320], val_loss: 11002.8574\n",
      "Epoch [2340], val_loss: 11003.1738\n",
      "Epoch [2360], val_loss: 11000.4863\n",
      "Epoch [2380], val_loss: 10991.6914\n",
      "Epoch [2400], val_loss: 10992.5488\n",
      "Epoch [2420], val_loss: 10992.7988\n",
      "Epoch [2440], val_loss: 10993.9844\n",
      "Epoch [2460], val_loss: 10994.1113\n",
      "Epoch [2480], val_loss: 10995.8564\n",
      "Epoch [2500], val_loss: 10987.3740\n",
      "Epoch [2520], val_loss: 10992.4727\n",
      "Epoch [2540], val_loss: 10984.2402\n",
      "Epoch [2560], val_loss: 10987.6348\n",
      "Epoch [2580], val_loss: 10986.9111\n",
      "Epoch [2600], val_loss: 10986.2881\n",
      "Epoch [2620], val_loss: 10985.1709\n",
      "Epoch [2640], val_loss: 10982.5156\n",
      "Epoch [2660], val_loss: 10981.9199\n",
      "Epoch [2680], val_loss: 10985.1895\n",
      "Epoch [2700], val_loss: 10989.9023\n",
      "Epoch [2720], val_loss: 10978.1787\n",
      "Epoch [2740], val_loss: 10979.3457\n",
      "Epoch [2760], val_loss: 10974.6426\n",
      "Epoch [2780], val_loss: 10981.1992\n",
      "Epoch [2800], val_loss: 10977.6846\n",
      "Epoch [2820], val_loss: 10977.8135\n",
      "Epoch [2840], val_loss: 10980.9287\n",
      "Epoch [2860], val_loss: 10982.4287\n",
      "Epoch [2880], val_loss: 10974.0400\n",
      "Epoch [2900], val_loss: 10973.2363\n",
      "Epoch [2920], val_loss: 10975.3652\n",
      "Epoch [2940], val_loss: 10969.9492\n",
      "Epoch [2960], val_loss: 10974.4600\n",
      "Epoch [2980], val_loss: 10970.7803\n",
      "Epoch [3000], val_loss: 10966.0283\n",
      "Epoch [3020], val_loss: 10969.9229\n",
      "Epoch [3040], val_loss: 10968.2070\n",
      "Epoch [3060], val_loss: 10962.8877\n",
      "Epoch [3080], val_loss: 10970.7920\n",
      "Epoch [3100], val_loss: 10964.0225\n",
      "Epoch [3120], val_loss: 10965.0195\n",
      "Epoch [3140], val_loss: 10968.4033\n",
      "Epoch [3160], val_loss: 10961.8809\n",
      "Epoch [3180], val_loss: 10959.0068\n",
      "Epoch [3200], val_loss: 10963.5293\n",
      "Epoch [3220], val_loss: 10966.4551\n",
      "Epoch [3240], val_loss: 10959.6104\n",
      "Epoch [3260], val_loss: 10958.0771\n",
      "Epoch [3280], val_loss: 10961.9004\n",
      "Epoch [3300], val_loss: 10955.1191\n",
      "Epoch [3320], val_loss: 10953.2988\n",
      "Epoch [3340], val_loss: 10953.7451\n",
      "Epoch [3360], val_loss: 10953.5098\n",
      "Epoch [3380], val_loss: 10954.9639\n",
      "Epoch [3400], val_loss: 10962.2686\n",
      "Epoch [3420], val_loss: 10958.3506\n",
      "Epoch [3440], val_loss: 10947.9551\n",
      "Epoch [3460], val_loss: 10954.2314\n",
      "Epoch [3480], val_loss: 10951.8984\n",
      "Epoch [3500], val_loss: 10950.3691\n",
      "Epoch [3520], val_loss: 10951.2207\n",
      "Epoch [3540], val_loss: 10950.8857\n",
      "Epoch [3560], val_loss: 10954.8789\n",
      "Epoch [3580], val_loss: 10949.4414\n",
      "Epoch [3600], val_loss: 10947.8633\n",
      "Epoch [3620], val_loss: 10950.0947\n",
      "Epoch [3640], val_loss: 10941.9277\n",
      "Epoch [3660], val_loss: 10945.4082\n",
      "Epoch [3680], val_loss: 10947.8320\n",
      "Epoch [3700], val_loss: 10943.0264\n",
      "Epoch [3720], val_loss: 10945.0049\n",
      "Epoch [3740], val_loss: 10942.5674\n",
      "Epoch [3760], val_loss: 10942.5947\n",
      "Epoch [3780], val_loss: 10943.1973\n",
      "Epoch [3800], val_loss: 10943.9756\n",
      "Epoch [3820], val_loss: 10938.5684\n",
      "Epoch [3840], val_loss: 10943.4951\n",
      "Epoch [3860], val_loss: 10945.4287\n",
      "Epoch [3880], val_loss: 10936.4141\n",
      "Epoch [3900], val_loss: 10938.4043\n",
      "Epoch [3920], val_loss: 10933.8232\n",
      "Epoch [3940], val_loss: 10938.7432\n",
      "Epoch [3960], val_loss: 10934.0547\n",
      "Epoch [3980], val_loss: 10935.2100\n",
      "Epoch [4000], val_loss: 10932.6113\n",
      "Epoch [4020], val_loss: 10937.1836\n",
      "Epoch [4040], val_loss: 10933.2285\n",
      "Epoch [4060], val_loss: 10933.5928\n",
      "Epoch [4080], val_loss: 10929.0674\n",
      "Epoch [4100], val_loss: 10927.5215\n",
      "Epoch [4120], val_loss: 10930.5059\n",
      "Epoch [4140], val_loss: 10925.7266\n",
      "Epoch [4160], val_loss: 10926.1904\n",
      "Epoch [4180], val_loss: 10927.5537\n",
      "Epoch [4200], val_loss: 10927.7969\n",
      "Epoch [4220], val_loss: 10927.1377\n",
      "Epoch [4240], val_loss: 10924.7227\n",
      "Epoch [4260], val_loss: 10927.1562\n",
      "Epoch [4280], val_loss: 10921.1230\n",
      "Epoch [4300], val_loss: 10920.4043\n",
      "Epoch [4320], val_loss: 10920.8311\n",
      "Epoch [4340], val_loss: 10920.5371\n",
      "Epoch [4360], val_loss: 10924.7207\n",
      "Epoch [4380], val_loss: 10923.5986\n",
      "Epoch [4400], val_loss: 10914.8965\n",
      "Epoch [4420], val_loss: 10920.0547\n",
      "Epoch [4440], val_loss: 10917.5225\n",
      "Epoch [4460], val_loss: 10918.1377\n",
      "Epoch [4480], val_loss: 10915.6904\n",
      "Epoch [4500], val_loss: 10918.2812\n",
      "Epoch [4520], val_loss: 10918.6855\n",
      "Epoch [4540], val_loss: 10911.2139\n",
      "Epoch [4560], val_loss: 10909.3057\n",
      "Epoch [4580], val_loss: 10912.0908\n",
      "Epoch [4600], val_loss: 10914.6270\n",
      "Epoch [4620], val_loss: 10910.1045\n",
      "Epoch [4640], val_loss: 10914.1328\n",
      "Epoch [4660], val_loss: 10911.1709\n",
      "Epoch [4680], val_loss: 10915.2197\n",
      "Epoch [4700], val_loss: 10907.8809\n",
      "Epoch [4720], val_loss: 10909.7305\n",
      "Epoch [4740], val_loss: 10905.8916\n",
      "Epoch [4760], val_loss: 10908.7637\n",
      "Epoch [4780], val_loss: 10910.5254\n",
      "Epoch [4800], val_loss: 10905.3379\n",
      "Epoch [4820], val_loss: 10903.8994\n",
      "Epoch [4840], val_loss: 10899.5713\n",
      "Epoch [4860], val_loss: 10901.6250\n",
      "Epoch [4880], val_loss: 10900.8926\n",
      "Epoch [4900], val_loss: 10904.0654\n",
      "Epoch [4920], val_loss: 10898.6719\n",
      "Epoch [4940], val_loss: 10903.0752\n",
      "Epoch [4960], val_loss: 10900.7119\n",
      "Epoch [4980], val_loss: 10901.3525\n",
      "Epoch [5000], val_loss: 10896.8047\n",
      "Epoch [5020], val_loss: 10893.2510\n",
      "Epoch [5040], val_loss: 10893.7188\n",
      "Epoch [5060], val_loss: 10894.3555\n",
      "Epoch [5080], val_loss: 10892.7598\n",
      "Epoch [5100], val_loss: 10899.5654\n",
      "Epoch [5120], val_loss: 10888.7656\n",
      "Epoch [5140], val_loss: 10892.2422\n",
      "Epoch [5160], val_loss: 10887.5430\n",
      "Epoch [5180], val_loss: 10892.7285\n",
      "Epoch [5200], val_loss: 10888.2686\n",
      "Epoch [5220], val_loss: 10889.7646\n",
      "Epoch [5240], val_loss: 10895.6904\n",
      "Epoch [5260], val_loss: 10888.6670\n",
      "Epoch [5280], val_loss: 10889.7402\n",
      "Epoch [5300], val_loss: 10891.2617\n",
      "Epoch [5320], val_loss: 10886.5254\n",
      "Epoch [5340], val_loss: 10885.1904\n",
      "Epoch [5360], val_loss: 10882.9082\n",
      "Epoch [5380], val_loss: 10878.9209\n",
      "Epoch [5400], val_loss: 10882.3594\n",
      "Epoch [5420], val_loss: 10888.5869\n",
      "Epoch [5440], val_loss: 10881.3291\n",
      "Epoch [5460], val_loss: 10879.6865\n",
      "Epoch [5480], val_loss: 10881.0752\n",
      "Epoch [5500], val_loss: 10880.0781\n",
      "Epoch [5520], val_loss: 10880.2412\n",
      "Epoch [5540], val_loss: 10875.4111\n",
      "Epoch [5560], val_loss: 10879.3877\n",
      "Epoch [5580], val_loss: 10877.0869\n",
      "Epoch [5600], val_loss: 10881.2344\n",
      "Epoch [5620], val_loss: 10878.6445\n",
      "Epoch [5640], val_loss: 10879.4961\n",
      "Epoch [5660], val_loss: 10878.8174\n",
      "Epoch [5680], val_loss: 10874.7998\n",
      "Epoch [5700], val_loss: 10873.9414\n",
      "Epoch [5720], val_loss: 10873.2256\n",
      "Epoch [5740], val_loss: 10875.7246\n",
      "Epoch [5760], val_loss: 10869.9873\n",
      "Epoch [5780], val_loss: 10873.7842\n",
      "Epoch [5800], val_loss: 10866.8857\n",
      "Epoch [5820], val_loss: 10867.5684\n",
      "Epoch [5840], val_loss: 10874.5977\n",
      "Epoch [5860], val_loss: 10871.0859\n",
      "Epoch [5880], val_loss: 10865.9229\n",
      "Epoch [5900], val_loss: 10865.5459\n",
      "Epoch [5920], val_loss: 10864.7598\n",
      "Epoch [5940], val_loss: 10876.1973\n",
      "Epoch [5960], val_loss: 10865.0098\n",
      "Epoch [5980], val_loss: 10868.5156\n",
      "Epoch [6000], val_loss: 10858.4883\n",
      "Epoch [6020], val_loss: 10864.1992\n",
      "Epoch [6040], val_loss: 10861.9053\n",
      "Epoch [6060], val_loss: 10867.5059\n",
      "Epoch [6080], val_loss: 10859.1777\n",
      "Epoch [6100], val_loss: 10862.9189\n",
      "Epoch [6120], val_loss: 10857.6924\n",
      "Epoch [6140], val_loss: 10856.4971\n",
      "Epoch [6160], val_loss: 10857.6348\n",
      "Epoch [6180], val_loss: 10859.3867\n",
      "Epoch [6200], val_loss: 10858.8867\n",
      "Epoch [6220], val_loss: 10856.7285\n",
      "Epoch [6240], val_loss: 10855.1504\n",
      "Epoch [6260], val_loss: 10857.1445\n",
      "Epoch [6280], val_loss: 10855.1221\n",
      "Epoch [6300], val_loss: 10854.7871\n",
      "Epoch [6320], val_loss: 10851.2441\n",
      "Epoch [6340], val_loss: 10854.8350\n",
      "Epoch [6360], val_loss: 10853.7520\n",
      "Epoch [6380], val_loss: 10850.9336\n",
      "Epoch [6400], val_loss: 10852.8574\n",
      "Epoch [6420], val_loss: 10847.1992\n",
      "Epoch [6440], val_loss: 10846.4297\n",
      "Epoch [6460], val_loss: 10846.7021\n",
      "Epoch [6480], val_loss: 10848.4600\n",
      "Epoch [6500], val_loss: 10844.2627\n",
      "Epoch [6520], val_loss: 10847.2021\n",
      "Epoch [6540], val_loss: 10843.3330\n",
      "Epoch [6560], val_loss: 10845.3867\n",
      "Epoch [6580], val_loss: 10845.2832\n",
      "Epoch [6600], val_loss: 10848.9111\n",
      "Epoch [6620], val_loss: 10843.4121\n",
      "Epoch [6640], val_loss: 10843.0029\n",
      "Epoch [6660], val_loss: 10839.5859\n",
      "Epoch [6680], val_loss: 10841.7432\n",
      "Epoch [6700], val_loss: 10843.9062\n",
      "Epoch [6720], val_loss: 10837.7432\n",
      "Epoch [6740], val_loss: 10836.4277\n",
      "Epoch [6760], val_loss: 10839.3203\n",
      "Epoch [6780], val_loss: 10836.7197\n",
      "Epoch [6800], val_loss: 10842.6689\n",
      "Epoch [6820], val_loss: 10835.0635\n",
      "Epoch [6840], val_loss: 10834.7910\n",
      "Epoch [6860], val_loss: 10836.1494\n",
      "Epoch [6880], val_loss: 10833.5752\n",
      "Epoch [6900], val_loss: 10834.1406\n",
      "Epoch [6920], val_loss: 10833.3271\n",
      "Epoch [6940], val_loss: 10833.0576\n",
      "Epoch [6960], val_loss: 10832.3936\n",
      "Epoch [6980], val_loss: 10830.4482\n",
      "Epoch [7000], val_loss: 10832.7354\n",
      "Epoch [7020], val_loss: 10832.3691\n",
      "Epoch [7040], val_loss: 10828.5127\n",
      "Epoch [7060], val_loss: 10828.3867\n",
      "Epoch [7080], val_loss: 10821.7656\n",
      "Epoch [7100], val_loss: 10832.2646\n",
      "Epoch [7120], val_loss: 10825.9844\n",
      "Epoch [7140], val_loss: 10824.3350\n",
      "Epoch [7160], val_loss: 10829.3262\n",
      "Epoch [7180], val_loss: 10825.8555\n",
      "Epoch [7200], val_loss: 10827.9893\n",
      "Epoch [7220], val_loss: 10824.1523\n",
      "Epoch [7240], val_loss: 10827.5830\n",
      "Epoch [7260], val_loss: 10820.6377\n",
      "Epoch [7280], val_loss: 10822.2012\n",
      "Epoch [7300], val_loss: 10818.6416\n",
      "Epoch [7320], val_loss: 10820.4072\n",
      "Epoch [7340], val_loss: 10821.1494\n",
      "Epoch [7360], val_loss: 10820.3545\n",
      "Epoch [7380], val_loss: 10815.5840\n",
      "Epoch [7400], val_loss: 10818.3818\n",
      "Epoch [7420], val_loss: 10814.4150\n",
      "Epoch [7440], val_loss: 10816.0615\n",
      "Epoch [7460], val_loss: 10815.0596\n",
      "Epoch [7480], val_loss: 10815.5449\n",
      "Epoch [7500], val_loss: 10820.7920\n",
      "Epoch [7520], val_loss: 10814.3750\n",
      "Epoch [7540], val_loss: 10811.9590\n",
      "Epoch [7560], val_loss: 10814.3174\n",
      "Epoch [7580], val_loss: 10818.2061\n",
      "Epoch [7600], val_loss: 10819.5225\n",
      "Epoch [7620], val_loss: 10811.4912\n",
      "Epoch [7640], val_loss: 10815.3838\n",
      "Epoch [7660], val_loss: 10811.8828\n",
      "Epoch [7680], val_loss: 10812.5977\n",
      "Epoch [7700], val_loss: 10805.4414\n",
      "Epoch [7720], val_loss: 10805.5176\n",
      "Epoch [7740], val_loss: 10807.4531\n",
      "Epoch [7760], val_loss: 10814.6855\n",
      "Epoch [7780], val_loss: 10803.0742\n",
      "Epoch [7800], val_loss: 10807.5146\n",
      "Epoch [7820], val_loss: 10802.5332\n",
      "Epoch [7840], val_loss: 10807.7207\n",
      "Epoch [7860], val_loss: 10803.8262\n",
      "Epoch [7880], val_loss: 10801.5967\n",
      "Epoch [7900], val_loss: 10804.2832\n",
      "Epoch [7920], val_loss: 10797.1816\n",
      "Epoch [7940], val_loss: 10805.0918\n",
      "Epoch [7960], val_loss: 10801.4570\n",
      "Epoch [7980], val_loss: 10798.4268\n",
      "Epoch [8000], val_loss: 10799.7461\n",
      "Epoch [8020], val_loss: 10800.4883\n",
      "Epoch [8040], val_loss: 10798.0850\n",
      "Epoch [8060], val_loss: 10797.6123\n",
      "Epoch [8080], val_loss: 10803.4492\n",
      "Epoch [8100], val_loss: 10795.0527\n",
      "Epoch [8120], val_loss: 10796.3271\n",
      "Epoch [8140], val_loss: 10794.3604\n",
      "Epoch [8160], val_loss: 10788.8916\n",
      "Epoch [8180], val_loss: 10795.0576\n",
      "Epoch [8200], val_loss: 10790.8525\n",
      "Epoch [8220], val_loss: 10790.7988\n",
      "Epoch [8240], val_loss: 10793.6328\n",
      "Epoch [8260], val_loss: 10793.1084\n",
      "Epoch [8280], val_loss: 10793.9150\n",
      "Epoch [8300], val_loss: 10784.9668\n",
      "Epoch [8320], val_loss: 10792.2725\n",
      "Epoch [8340], val_loss: 10786.7783\n",
      "Epoch [8360], val_loss: 10790.8252\n",
      "Epoch [8380], val_loss: 10787.4932\n",
      "Epoch [8400], val_loss: 10788.9111\n",
      "Epoch [8420], val_loss: 10788.4619\n",
      "Epoch [8440], val_loss: 10788.4922\n",
      "Epoch [8460], val_loss: 10783.1895\n",
      "Epoch [8480], val_loss: 10784.5303\n",
      "Epoch [8500], val_loss: 10780.0605\n",
      "Epoch [8520], val_loss: 10779.3604\n",
      "Epoch [8540], val_loss: 10781.7900\n",
      "Epoch [8560], val_loss: 10784.1133\n",
      "Epoch [8580], val_loss: 10786.0576\n",
      "Epoch [8600], val_loss: 10780.5469\n",
      "Epoch [8620], val_loss: 10776.5156\n",
      "Epoch [8640], val_loss: 10777.4912\n",
      "Epoch [8660], val_loss: 10784.2588\n",
      "Epoch [8680], val_loss: 10776.7090\n",
      "Epoch [8700], val_loss: 10774.4297\n",
      "Epoch [8720], val_loss: 10772.4727\n",
      "Epoch [8740], val_loss: 10771.4922\n",
      "Epoch [8760], val_loss: 10770.8916\n",
      "Epoch [8780], val_loss: 10779.7832\n",
      "Epoch [8800], val_loss: 10771.2852\n",
      "Epoch [8820], val_loss: 10776.6221\n",
      "Epoch [8840], val_loss: 10771.2354\n",
      "Epoch [8860], val_loss: 10767.5283\n",
      "Epoch [8880], val_loss: 10770.7549\n",
      "Epoch [8900], val_loss: 10764.5400\n",
      "Epoch [8920], val_loss: 10769.5820\n",
      "Epoch [8940], val_loss: 10767.0811\n",
      "Epoch [8960], val_loss: 10770.5410\n",
      "Epoch [8980], val_loss: 10760.5010\n",
      "Epoch [9000], val_loss: 10771.1953\n",
      "Epoch [9020], val_loss: 10769.0195\n",
      "Epoch [9040], val_loss: 10760.4775\n",
      "Epoch [9060], val_loss: 10763.1738\n",
      "Epoch [9080], val_loss: 10767.6963\n",
      "Epoch [9100], val_loss: 10763.7021\n",
      "Epoch [9120], val_loss: 10763.8271\n",
      "Epoch [9140], val_loss: 10765.2949\n",
      "Epoch [9160], val_loss: 10759.6201\n",
      "Epoch [9180], val_loss: 10763.1084\n",
      "Epoch [9200], val_loss: 10758.5859\n",
      "Epoch [9220], val_loss: 10764.2012\n",
      "Epoch [9240], val_loss: 10765.2178\n",
      "Epoch [9260], val_loss: 10760.4814\n",
      "Epoch [9280], val_loss: 10767.2490\n",
      "Epoch [9300], val_loss: 10760.8975\n",
      "Epoch [9320], val_loss: 10749.5605\n",
      "Epoch [9340], val_loss: 10758.4023\n",
      "Epoch [9360], val_loss: 10753.4180\n",
      "Epoch [9380], val_loss: 10754.7529\n",
      "Epoch [9400], val_loss: 10759.8564\n",
      "Epoch [9420], val_loss: 10750.6826\n",
      "Epoch [9440], val_loss: 10751.9609\n",
      "Epoch [9460], val_loss: 10754.4893\n",
      "Epoch [9480], val_loss: 10752.5029\n",
      "Epoch [9500], val_loss: 10749.9209\n",
      "Epoch [9520], val_loss: 10760.6768\n",
      "Epoch [9540], val_loss: 10747.0166\n",
      "Epoch [9560], val_loss: 10752.7441\n",
      "Epoch [9580], val_loss: 10746.5859\n",
      "Epoch [9600], val_loss: 10742.5664\n",
      "Epoch [9620], val_loss: 10750.0273\n",
      "Epoch [9640], val_loss: 10740.8486\n",
      "Epoch [9660], val_loss: 10746.2061\n",
      "Epoch [9680], val_loss: 10745.8604\n",
      "Epoch [9700], val_loss: 10743.0732\n",
      "Epoch [9720], val_loss: 10749.1602\n",
      "Epoch [9740], val_loss: 10744.4297\n",
      "Epoch [9760], val_loss: 10737.3877\n",
      "Epoch [9780], val_loss: 10741.0859\n",
      "Epoch [9800], val_loss: 10738.9541\n",
      "Epoch [9820], val_loss: 10745.5586\n",
      "Epoch [9840], val_loss: 10739.5898\n",
      "Epoch [9860], val_loss: 10740.7314\n",
      "Epoch [9880], val_loss: 10738.7920\n",
      "Epoch [9900], val_loss: 10744.3838\n",
      "Epoch [9920], val_loss: 10736.3047\n",
      "Epoch [9940], val_loss: 10736.7061\n",
      "Epoch [9960], val_loss: 10735.2686\n",
      "Epoch [9980], val_loss: 10739.8076\n",
      "Epoch [10000], val_loss: 10734.7188\n",
      "Epoch [10020], val_loss: 10732.5977\n",
      "Epoch [10040], val_loss: 10739.0176\n",
      "Epoch [10060], val_loss: 10733.5303\n",
      "Epoch [10080], val_loss: 10733.5283\n",
      "Epoch [10100], val_loss: 10727.0010\n",
      "Epoch [10120], val_loss: 10736.5986\n",
      "Epoch [10140], val_loss: 10733.8135\n",
      "Epoch [10160], val_loss: 10736.3809\n",
      "Epoch [10180], val_loss: 10731.6328\n",
      "Epoch [10200], val_loss: 10735.6641\n",
      "Epoch [10220], val_loss: 10730.6914\n",
      "Epoch [10240], val_loss: 10726.2285\n",
      "Epoch [10260], val_loss: 10724.3457\n",
      "Epoch [10280], val_loss: 10730.2080\n",
      "Epoch [10300], val_loss: 10728.1396\n",
      "Epoch [10320], val_loss: 10728.1826\n",
      "Epoch [10340], val_loss: 10723.4492\n",
      "Epoch [10360], val_loss: 10727.3965\n",
      "Epoch [10380], val_loss: 10727.5020\n",
      "Epoch [10400], val_loss: 10715.6836\n",
      "Epoch [10420], val_loss: 10721.8008\n",
      "Epoch [10440], val_loss: 10722.8652\n",
      "Epoch [10460], val_loss: 10716.2900\n",
      "Epoch [10480], val_loss: 10727.9131\n",
      "Epoch [10500], val_loss: 10715.6934\n",
      "Epoch [10520], val_loss: 10722.6123\n",
      "Epoch [10540], val_loss: 10715.5586\n",
      "Epoch [10560], val_loss: 10716.8223\n",
      "Epoch [10580], val_loss: 10720.4824\n",
      "Epoch [10600], val_loss: 10714.3486\n",
      "Epoch [10620], val_loss: 10719.6279\n",
      "Epoch [10640], val_loss: 10707.0986\n",
      "Epoch [10660], val_loss: 10713.2334\n",
      "Epoch [10680], val_loss: 10714.9893\n",
      "Epoch [10700], val_loss: 10713.8799\n",
      "Epoch [10720], val_loss: 10710.3291\n",
      "Epoch [10740], val_loss: 10711.0645\n",
      "Epoch [10760], val_loss: 10717.3936\n",
      "Epoch [10780], val_loss: 10715.8994\n",
      "Epoch [10800], val_loss: 10706.1855\n",
      "Epoch [10820], val_loss: 10708.6338\n",
      "Epoch [10840], val_loss: 10712.6230\n",
      "Epoch [10860], val_loss: 10712.5811\n",
      "Epoch [10880], val_loss: 10709.8877\n",
      "Epoch [10900], val_loss: 10705.9307\n",
      "Epoch [10920], val_loss: 10715.1484\n",
      "Epoch [10940], val_loss: 10705.7539\n",
      "Epoch [10960], val_loss: 10710.2256\n",
      "Epoch [10980], val_loss: 10699.2236\n",
      "Epoch [11000], val_loss: 10707.1436\n",
      "Epoch [11020], val_loss: 10704.7783\n",
      "Epoch [11040], val_loss: 10706.0469\n",
      "Epoch [11060], val_loss: 10701.9814\n",
      "Epoch [11080], val_loss: 10700.8281\n",
      "Epoch [11100], val_loss: 10706.7061\n",
      "Epoch [11120], val_loss: 10696.3135\n",
      "Epoch [11140], val_loss: 10708.1055\n",
      "Epoch [11160], val_loss: 10706.4717\n",
      "Epoch [11180], val_loss: 10697.9580\n",
      "Epoch [11200], val_loss: 10699.1719\n",
      "Epoch [11220], val_loss: 10694.4902\n",
      "Epoch [11240], val_loss: 10698.3564\n",
      "Epoch [11260], val_loss: 10691.3955\n",
      "Epoch [11280], val_loss: 10696.3799\n",
      "Epoch [11300], val_loss: 10694.5420\n",
      "Epoch [11320], val_loss: 10693.8721\n",
      "Epoch [11340], val_loss: 10690.5879\n",
      "Epoch [11360], val_loss: 10691.4336\n",
      "Epoch [11380], val_loss: 10695.7871\n",
      "Epoch [11400], val_loss: 10695.0215\n",
      "Epoch [11420], val_loss: 10687.5127\n",
      "Epoch [11440], val_loss: 10687.5918\n",
      "Epoch [11460], val_loss: 10687.7559\n",
      "Epoch [11480], val_loss: 10694.4141\n",
      "Epoch [11500], val_loss: 10688.3711\n",
      "Epoch [11520], val_loss: 10683.0869\n",
      "Epoch [11540], val_loss: 10695.2500\n",
      "Epoch [11560], val_loss: 10690.1035\n",
      "Epoch [11580], val_loss: 10683.3672\n",
      "Epoch [11600], val_loss: 10685.8613\n",
      "Epoch [11620], val_loss: 10681.7285\n",
      "Epoch [11640], val_loss: 10685.7900\n",
      "Epoch [11660], val_loss: 10685.4775\n",
      "Epoch [11680], val_loss: 10686.3330\n",
      "Epoch [11700], val_loss: 10684.2744\n",
      "Epoch [11720], val_loss: 10682.7148\n",
      "Epoch [11740], val_loss: 10682.7793\n",
      "Epoch [11760], val_loss: 10679.2109\n",
      "Epoch [11780], val_loss: 10677.9482\n",
      "Epoch [11800], val_loss: 10679.3447\n",
      "Epoch [11820], val_loss: 10689.1953\n",
      "Epoch [11840], val_loss: 10678.7100\n",
      "Epoch [11860], val_loss: 10677.1396\n",
      "Epoch [11880], val_loss: 10678.7295\n",
      "Epoch [11900], val_loss: 10673.0254\n",
      "Epoch [11920], val_loss: 10679.7891\n",
      "Epoch [11940], val_loss: 10671.3486\n",
      "Epoch [11960], val_loss: 10678.3291\n",
      "Epoch [11980], val_loss: 10673.6250\n",
      "Epoch [12000], val_loss: 10682.6553\n",
      "Epoch [12020], val_loss: 10671.6953\n",
      "Epoch [12040], val_loss: 10674.1074\n",
      "Epoch [12060], val_loss: 10676.5820\n",
      "Epoch [12080], val_loss: 10667.6035\n",
      "Epoch [12100], val_loss: 10667.2588\n",
      "Epoch [12120], val_loss: 10661.8672\n",
      "Epoch [12140], val_loss: 10668.4355\n",
      "Epoch [12160], val_loss: 10666.7637\n",
      "Epoch [12180], val_loss: 10662.6523\n",
      "Epoch [12200], val_loss: 10665.8896\n",
      "Epoch [12220], val_loss: 10666.7109\n",
      "Epoch [12240], val_loss: 10673.1270\n",
      "Epoch [12260], val_loss: 10661.4697\n",
      "Epoch [12280], val_loss: 10663.4668\n",
      "Epoch [12300], val_loss: 10671.0479\n",
      "Epoch [12320], val_loss: 10664.8115\n",
      "Epoch [12340], val_loss: 10660.3438\n",
      "Epoch [12360], val_loss: 10667.4404\n",
      "Epoch [12380], val_loss: 10655.5645\n",
      "Epoch [12400], val_loss: 10656.6318\n",
      "Epoch [12420], val_loss: 10655.7998\n",
      "Epoch [12440], val_loss: 10661.3662\n",
      "Epoch [12460], val_loss: 10664.1221\n",
      "Epoch [12480], val_loss: 10654.2207\n",
      "Epoch [12500], val_loss: 10657.1553\n",
      "Epoch [12520], val_loss: 10656.3955\n",
      "Epoch [12540], val_loss: 10656.3643\n",
      "Epoch [12560], val_loss: 10663.3652\n",
      "Epoch [12580], val_loss: 10659.9248\n",
      "Epoch [12600], val_loss: 10654.7295\n",
      "Epoch [12620], val_loss: 10658.7646\n",
      "Epoch [12640], val_loss: 10651.3145\n",
      "Epoch [12660], val_loss: 10650.0068\n",
      "Epoch [12680], val_loss: 10656.0840\n",
      "Epoch [12700], val_loss: 10648.4336\n",
      "Epoch [12720], val_loss: 10649.6387\n",
      "Epoch [12740], val_loss: 10647.5811\n",
      "Epoch [12760], val_loss: 10651.7305\n",
      "Epoch [12780], val_loss: 10648.7900\n",
      "Epoch [12800], val_loss: 10654.5410\n",
      "Epoch [12820], val_loss: 10651.9365\n",
      "Epoch [12840], val_loss: 10640.8105\n",
      "Epoch [12860], val_loss: 10640.7314\n",
      "Epoch [12880], val_loss: 10644.8027\n",
      "Epoch [12900], val_loss: 10649.5703\n",
      "Epoch [12920], val_loss: 10641.6924\n",
      "Epoch [12940], val_loss: 10645.9580\n",
      "Epoch [12960], val_loss: 10643.1562\n",
      "Epoch [12980], val_loss: 10637.2295\n",
      "Epoch [13000], val_loss: 10641.4141\n",
      "Epoch [13020], val_loss: 10647.0557\n",
      "Epoch [13040], val_loss: 10638.0430\n",
      "Epoch [13060], val_loss: 10634.3799\n",
      "Epoch [13080], val_loss: 10636.6523\n",
      "Epoch [13100], val_loss: 10639.3740\n",
      "Epoch [13120], val_loss: 10632.9551\n",
      "Epoch [13140], val_loss: 10636.7402\n",
      "Epoch [13160], val_loss: 10635.2900\n",
      "Epoch [13180], val_loss: 10633.4512\n",
      "Epoch [13200], val_loss: 10639.2930\n",
      "Epoch [13220], val_loss: 10635.9814\n",
      "Epoch [13240], val_loss: 10639.1113\n",
      "Epoch [13260], val_loss: 10638.4443\n",
      "Epoch [13280], val_loss: 10634.9697\n",
      "Epoch [13300], val_loss: 10633.3340\n",
      "Epoch [13320], val_loss: 10633.8115\n",
      "Epoch [13340], val_loss: 10630.0283\n",
      "Epoch [13360], val_loss: 10625.8896\n",
      "Epoch [13380], val_loss: 10630.4443\n",
      "Epoch [13400], val_loss: 10630.1836\n",
      "Epoch [13420], val_loss: 10624.5742\n",
      "Epoch [13440], val_loss: 10625.5029\n",
      "Epoch [13460], val_loss: 10625.1260\n",
      "Epoch [13480], val_loss: 10627.0664\n",
      "Epoch [13500], val_loss: 10628.3945\n",
      "Epoch [13520], val_loss: 10620.7783\n",
      "Epoch [13540], val_loss: 10617.4307\n",
      "Epoch [13560], val_loss: 10622.6836\n",
      "Epoch [13580], val_loss: 10627.3193\n",
      "Epoch [13600], val_loss: 10622.5527\n",
      "Epoch [13620], val_loss: 10623.9268\n",
      "Epoch [13640], val_loss: 10620.8799\n",
      "Epoch [13660], val_loss: 10622.4385\n",
      "Epoch [13680], val_loss: 10617.6533\n",
      "Epoch [13700], val_loss: 10619.1396\n",
      "Epoch [13720], val_loss: 10617.4326\n",
      "Epoch [13740], val_loss: 10615.2881\n",
      "Epoch [13760], val_loss: 10615.8721\n",
      "Epoch [13780], val_loss: 10619.8838\n",
      "Epoch [13800], val_loss: 10612.8535\n",
      "Epoch [13820], val_loss: 10613.9688\n",
      "Epoch [13840], val_loss: 10615.7705\n",
      "Epoch [13860], val_loss: 10615.9580\n",
      "Epoch [13880], val_loss: 10617.7715\n",
      "Epoch [13900], val_loss: 10612.8008\n",
      "Epoch [13920], val_loss: 10615.9170\n",
      "Epoch [13940], val_loss: 10615.3564\n",
      "Epoch [13960], val_loss: 10618.7217\n",
      "Epoch [13980], val_loss: 10612.1475\n",
      "Epoch [14000], val_loss: 10615.7549\n",
      "Epoch [14020], val_loss: 10606.3789\n",
      "Epoch [14040], val_loss: 10608.4707\n",
      "Epoch [14060], val_loss: 10609.7139\n",
      "Epoch [14080], val_loss: 10605.7197\n",
      "Epoch [14100], val_loss: 10608.5820\n",
      "Epoch [14120], val_loss: 10601.7559\n",
      "Epoch [14140], val_loss: 10608.7568\n",
      "Epoch [14160], val_loss: 10609.0059\n",
      "Epoch [14180], val_loss: 10608.2148\n",
      "Epoch [14200], val_loss: 10606.2607\n",
      "Epoch [14220], val_loss: 10600.0000\n",
      "Epoch [14240], val_loss: 10602.7988\n",
      "Epoch [14260], val_loss: 10600.3643\n",
      "Epoch [14280], val_loss: 10602.6826\n",
      "Epoch [14300], val_loss: 10600.3760\n",
      "Epoch [14320], val_loss: 10596.9307\n",
      "Epoch [14340], val_loss: 10600.6455\n",
      "Epoch [14360], val_loss: 10594.1855\n",
      "Epoch [14380], val_loss: 10600.0410\n",
      "Epoch [14400], val_loss: 10592.5195\n",
      "Epoch [14420], val_loss: 10595.0986\n",
      "Epoch [14440], val_loss: 10595.9307\n",
      "Epoch [14460], val_loss: 10596.7959\n",
      "Epoch [14480], val_loss: 10592.8174\n",
      "Epoch [14500], val_loss: 10592.7822\n",
      "Epoch [14520], val_loss: 10590.7695\n",
      "Epoch [14540], val_loss: 10597.7998\n",
      "Epoch [14560], val_loss: 10595.6465\n",
      "Epoch [14580], val_loss: 10595.2764\n",
      "Epoch [14600], val_loss: 10594.3340\n",
      "Epoch [14620], val_loss: 10594.3213\n",
      "Epoch [14640], val_loss: 10586.5850\n",
      "Epoch [14660], val_loss: 10591.4160\n",
      "Epoch [14680], val_loss: 10590.0586\n",
      "Epoch [14700], val_loss: 10597.2363\n",
      "Epoch [14720], val_loss: 10587.5381\n",
      "Epoch [14740], val_loss: 10587.8516\n",
      "Epoch [14760], val_loss: 10586.4434\n",
      "Epoch [14780], val_loss: 10587.7734\n",
      "Epoch [14800], val_loss: 10584.8467\n",
      "Epoch [14820], val_loss: 10581.3984\n",
      "Epoch [14840], val_loss: 10581.5684\n",
      "Epoch [14860], val_loss: 10581.7285\n",
      "Epoch [14880], val_loss: 10585.4707\n",
      "Epoch [14900], val_loss: 10580.8203\n",
      "Epoch [14920], val_loss: 10583.4795\n",
      "Epoch [14940], val_loss: 10587.5645\n",
      "Epoch [14960], val_loss: 10582.0938\n",
      "Epoch [14980], val_loss: 10582.0039\n",
      "Epoch [15000], val_loss: 10581.0869\n",
      "Epoch [15020], val_loss: 10573.9785\n",
      "Epoch [15040], val_loss: 10579.8164\n",
      "Epoch [15060], val_loss: 10579.3398\n",
      "Epoch [15080], val_loss: 10573.0645\n",
      "Epoch [15100], val_loss: 10574.4033\n",
      "Epoch [15120], val_loss: 10572.4248\n",
      "Epoch [15140], val_loss: 10578.1152\n",
      "Epoch [15160], val_loss: 10570.6006\n",
      "Epoch [15180], val_loss: 10570.6279\n",
      "Epoch [15200], val_loss: 10576.8623\n",
      "Epoch [15220], val_loss: 10577.1973\n",
      "Epoch [15240], val_loss: 10565.0752\n",
      "Epoch [15260], val_loss: 10569.9961\n",
      "Epoch [15280], val_loss: 10571.0244\n",
      "Epoch [15300], val_loss: 10566.9111\n",
      "Epoch [15320], val_loss: 10569.9971\n",
      "Epoch [15340], val_loss: 10569.3164\n",
      "Epoch [15360], val_loss: 10569.4336\n",
      "Epoch [15380], val_loss: 10569.1328\n",
      "Epoch [15400], val_loss: 10567.4766\n",
      "Epoch [15420], val_loss: 10564.9668\n",
      "Epoch [15440], val_loss: 10559.1865\n",
      "Epoch [15460], val_loss: 10560.0654\n",
      "Epoch [15480], val_loss: 10558.7598\n",
      "Epoch [15500], val_loss: 10563.5967\n",
      "Epoch [15520], val_loss: 10560.1162\n",
      "Epoch [15540], val_loss: 10557.8613\n",
      "Epoch [15560], val_loss: 10567.7988\n",
      "Epoch [15580], val_loss: 10554.7803\n",
      "Epoch [15600], val_loss: 10557.6084\n",
      "Epoch [15620], val_loss: 10562.4004\n",
      "Epoch [15640], val_loss: 10556.8535\n",
      "Epoch [15660], val_loss: 10555.4756\n",
      "Epoch [15680], val_loss: 10562.9111\n",
      "Epoch [15700], val_loss: 10555.1680\n",
      "Epoch [15720], val_loss: 10558.9805\n",
      "Epoch [15740], val_loss: 10556.8496\n",
      "Epoch [15760], val_loss: 10554.0391\n",
      "Epoch [15780], val_loss: 10560.0225\n",
      "Epoch [15800], val_loss: 10554.5059\n",
      "Epoch [15820], val_loss: 10550.6328\n",
      "Epoch [15840], val_loss: 10551.7275\n",
      "Epoch [15860], val_loss: 10549.7783\n",
      "Epoch [15880], val_loss: 10550.4082\n",
      "Epoch [15900], val_loss: 10560.8945\n",
      "Epoch [15920], val_loss: 10550.4570\n",
      "Epoch [15940], val_loss: 10547.5225\n",
      "Epoch [15960], val_loss: 10553.0127\n",
      "Epoch [15980], val_loss: 10545.6768\n",
      "Epoch [16000], val_loss: 10548.0869\n",
      "Epoch [16020], val_loss: 10546.6270\n",
      "Epoch [16040], val_loss: 10546.0000\n",
      "Epoch [16060], val_loss: 10538.1924\n",
      "Epoch [16080], val_loss: 10543.6357\n",
      "Epoch [16100], val_loss: 10545.1191\n",
      "Epoch [16120], val_loss: 10542.7178\n",
      "Epoch [16140], val_loss: 10545.3203\n",
      "Epoch [16160], val_loss: 10547.5762\n",
      "Epoch [16180], val_loss: 10543.0977\n",
      "Epoch [16200], val_loss: 10540.9043\n",
      "Epoch [16220], val_loss: 10541.4365\n",
      "Epoch [16240], val_loss: 10540.7852\n",
      "Epoch [16260], val_loss: 10537.5322\n",
      "Epoch [16280], val_loss: 10545.0098\n",
      "Epoch [16300], val_loss: 10543.7598\n",
      "Epoch [16320], val_loss: 10536.7900\n",
      "Epoch [16340], val_loss: 10536.4727\n",
      "Epoch [16360], val_loss: 10533.8633\n",
      "Epoch [16380], val_loss: 10539.9941\n",
      "Epoch [16400], val_loss: 10532.4248\n",
      "Epoch [16420], val_loss: 10535.2480\n",
      "Epoch [16440], val_loss: 10531.3721\n",
      "Epoch [16460], val_loss: 10532.4219\n",
      "Epoch [16480], val_loss: 10531.8564\n",
      "Epoch [16500], val_loss: 10538.8623\n",
      "Epoch [16520], val_loss: 10532.6748\n",
      "Epoch [16540], val_loss: 10533.0361\n",
      "Epoch [16560], val_loss: 10527.5430\n",
      "Epoch [16580], val_loss: 10526.9277\n",
      "Epoch [16600], val_loss: 10529.9561\n",
      "Epoch [16620], val_loss: 10530.7588\n",
      "Epoch [16640], val_loss: 10528.9180\n",
      "Epoch [16660], val_loss: 10522.8232\n",
      "Epoch [16680], val_loss: 10526.7871\n",
      "Epoch [16700], val_loss: 10527.8135\n",
      "Epoch [16720], val_loss: 10528.9980\n",
      "Epoch [16740], val_loss: 10527.2822\n",
      "Epoch [16760], val_loss: 10523.7158\n",
      "Epoch [16780], val_loss: 10520.7383\n",
      "Epoch [16800], val_loss: 10523.3877\n",
      "Epoch [16820], val_loss: 10522.6328\n",
      "Epoch [16840], val_loss: 10521.2393\n",
      "Epoch [16860], val_loss: 10520.9121\n",
      "Epoch [16880], val_loss: 10516.8945\n",
      "Epoch [16900], val_loss: 10520.1299\n",
      "Epoch [16920], val_loss: 10517.2393\n",
      "Epoch [16940], val_loss: 10515.8672\n",
      "Epoch [16960], val_loss: 10519.0781\n",
      "Epoch [16980], val_loss: 10513.3174\n",
      "Epoch [17000], val_loss: 10521.5000\n",
      "Epoch [17020], val_loss: 10509.9854\n",
      "Epoch [17040], val_loss: 10512.1943\n",
      "Epoch [17060], val_loss: 10516.3477\n",
      "Epoch [17080], val_loss: 10516.4414\n",
      "Epoch [17100], val_loss: 10510.5723\n",
      "Epoch [17120], val_loss: 10513.9023\n",
      "Epoch [17140], val_loss: 10508.6611\n",
      "Epoch [17160], val_loss: 10514.2920\n",
      "Epoch [17180], val_loss: 10515.5947\n",
      "Epoch [17200], val_loss: 10508.4189\n",
      "Epoch [17220], val_loss: 10509.6475\n",
      "Epoch [17240], val_loss: 10508.2812\n",
      "Epoch [17260], val_loss: 10505.6387\n",
      "Epoch [17280], val_loss: 10504.2031\n",
      "Epoch [17300], val_loss: 10506.8076\n",
      "Epoch [17320], val_loss: 10505.5811\n",
      "Epoch [17340], val_loss: 10511.5811\n",
      "Epoch [17360], val_loss: 10503.9951\n",
      "Epoch [17380], val_loss: 10506.3770\n",
      "Epoch [17400], val_loss: 10503.6172\n",
      "Epoch [17420], val_loss: 10503.5361\n",
      "Epoch [17440], val_loss: 10501.5098\n",
      "Epoch [17460], val_loss: 10500.1270\n",
      "Epoch [17480], val_loss: 10501.2158\n",
      "Epoch [17500], val_loss: 10503.1182\n",
      "Epoch [17520], val_loss: 10500.1416\n",
      "Epoch [17540], val_loss: 10500.4922\n",
      "Epoch [17560], val_loss: 10498.7500\n",
      "Epoch [17580], val_loss: 10496.2207\n",
      "Epoch [17600], val_loss: 10506.5967\n",
      "Epoch [17620], val_loss: 10496.2656\n",
      "Epoch [17640], val_loss: 10493.6484\n",
      "Epoch [17660], val_loss: 10495.9561\n",
      "Epoch [17680], val_loss: 10500.1748\n",
      "Epoch [17700], val_loss: 10496.3750\n",
      "Epoch [17720], val_loss: 10496.4561\n",
      "Epoch [17740], val_loss: 10497.0264\n",
      "Epoch [17760], val_loss: 10493.1338\n",
      "Epoch [17780], val_loss: 10495.6240\n",
      "Epoch [17800], val_loss: 10490.1553\n",
      "Epoch [17820], val_loss: 10488.2578\n",
      "Epoch [17840], val_loss: 10493.7012\n",
      "Epoch [17860], val_loss: 10487.8887\n",
      "Epoch [17880], val_loss: 10488.7920\n",
      "Epoch [17900], val_loss: 10485.6992\n",
      "Epoch [17920], val_loss: 10487.1504\n",
      "Epoch [17940], val_loss: 10485.9746\n",
      "Epoch [17960], val_loss: 10482.3164\n",
      "Epoch [17980], val_loss: 10480.9316\n",
      "Epoch [18000], val_loss: 10485.4209\n",
      "Epoch [18020], val_loss: 10486.3691\n",
      "Epoch [18040], val_loss: 10479.5957\n",
      "Epoch [18060], val_loss: 10485.0635\n",
      "Epoch [18080], val_loss: 10480.9990\n",
      "Epoch [18100], val_loss: 10480.0176\n",
      "Epoch [18120], val_loss: 10483.2871\n",
      "Epoch [18140], val_loss: 10480.9102\n",
      "Epoch [18160], val_loss: 10479.5391\n",
      "Epoch [18180], val_loss: 10473.9814\n",
      "Epoch [18200], val_loss: 10479.2197\n",
      "Epoch [18220], val_loss: 10476.4141\n",
      "Epoch [18240], val_loss: 10476.7686\n",
      "Epoch [18260], val_loss: 10481.7646\n",
      "Epoch [18280], val_loss: 10481.0557\n",
      "Epoch [18300], val_loss: 10474.9736\n",
      "Epoch [18320], val_loss: 10476.4912\n",
      "Epoch [18340], val_loss: 10476.9697\n",
      "Epoch [18360], val_loss: 10472.7158\n",
      "Epoch [18380], val_loss: 10473.3457\n",
      "Epoch [18400], val_loss: 10470.2314\n",
      "Epoch [18420], val_loss: 10478.4668\n",
      "Epoch [18440], val_loss: 10474.8945\n",
      "Epoch [18460], val_loss: 10474.8613\n",
      "Epoch [18480], val_loss: 10470.2539\n",
      "Epoch [18500], val_loss: 10473.2842\n",
      "Epoch [18520], val_loss: 10471.7998\n",
      "Epoch [18540], val_loss: 10470.1328\n",
      "Epoch [18560], val_loss: 10461.4941\n",
      "Epoch [18580], val_loss: 10472.5957\n",
      "Epoch [18600], val_loss: 10465.2715\n",
      "Epoch [18620], val_loss: 10463.6260\n",
      "Epoch [18640], val_loss: 10468.4922\n",
      "Epoch [18660], val_loss: 10461.9609\n",
      "Epoch [18680], val_loss: 10462.8867\n",
      "Epoch [18700], val_loss: 10466.1553\n",
      "Epoch [18720], val_loss: 10466.1328\n",
      "Epoch [18740], val_loss: 10460.6689\n",
      "Epoch [18760], val_loss: 10459.9297\n",
      "Epoch [18780], val_loss: 10457.2188\n",
      "Epoch [18800], val_loss: 10457.1797\n",
      "Epoch [18820], val_loss: 10456.2637\n",
      "Epoch [18840], val_loss: 10463.9814\n",
      "Epoch [18860], val_loss: 10455.2656\n",
      "Epoch [18880], val_loss: 10462.1826\n",
      "Epoch [18900], val_loss: 10461.0029\n",
      "Epoch [18920], val_loss: 10453.1846\n",
      "Epoch [18940], val_loss: 10459.5303\n",
      "Epoch [18960], val_loss: 10454.6553\n",
      "Epoch [18980], val_loss: 10460.4307\n",
      "Epoch [19000], val_loss: 10451.2686\n",
      "Epoch [19020], val_loss: 10448.9844\n",
      "Epoch [19040], val_loss: 10456.8535\n",
      "Epoch [19060], val_loss: 10451.7969\n",
      "Epoch [19080], val_loss: 10455.2812\n",
      "Epoch [19100], val_loss: 10450.4570\n",
      "Epoch [19120], val_loss: 10447.4492\n",
      "Epoch [19140], val_loss: 10453.2148\n",
      "Epoch [19160], val_loss: 10449.7256\n",
      "Epoch [19180], val_loss: 10446.8057\n",
      "Epoch [19200], val_loss: 10446.8271\n",
      "Epoch [19220], val_loss: 10448.9385\n",
      "Epoch [19240], val_loss: 10448.5840\n",
      "Epoch [19260], val_loss: 10444.6758\n",
      "Epoch [19280], val_loss: 10445.6182\n",
      "Epoch [19300], val_loss: 10446.9072\n",
      "Epoch [19320], val_loss: 10438.8770\n",
      "Epoch [19340], val_loss: 10445.7686\n",
      "Epoch [19360], val_loss: 10447.1572\n",
      "Epoch [19380], val_loss: 10441.0254\n",
      "Epoch [19400], val_loss: 10437.8301\n",
      "Epoch [19420], val_loss: 10438.6826\n",
      "Epoch [19440], val_loss: 10437.2266\n",
      "Epoch [19460], val_loss: 10436.8408\n",
      "Epoch [19480], val_loss: 10442.3311\n",
      "Epoch [19500], val_loss: 10439.9473\n",
      "Epoch [19520], val_loss: 10437.0469\n",
      "Epoch [19540], val_loss: 10442.2529\n",
      "Epoch [19560], val_loss: 10433.7373\n",
      "Epoch [19580], val_loss: 10436.4004\n",
      "Epoch [19600], val_loss: 10435.8281\n",
      "Epoch [19620], val_loss: 10433.3320\n",
      "Epoch [19640], val_loss: 10435.6562\n",
      "Epoch [19660], val_loss: 10438.4551\n",
      "Epoch [19680], val_loss: 10433.1904\n",
      "Epoch [19700], val_loss: 10440.1943\n",
      "Epoch [19720], val_loss: 10432.2012\n",
      "Epoch [19740], val_loss: 10428.1836\n",
      "Epoch [19760], val_loss: 10430.3252\n",
      "Epoch [19780], val_loss: 10434.1260\n",
      "Epoch [19800], val_loss: 10425.6016\n",
      "Epoch [19820], val_loss: 10430.9346\n",
      "Epoch [19840], val_loss: 10426.3389\n",
      "Epoch [19860], val_loss: 10431.4512\n",
      "Epoch [19880], val_loss: 10431.8652\n",
      "Epoch [19900], val_loss: 10427.6475\n",
      "Epoch [19920], val_loss: 10425.2334\n",
      "Epoch [19940], val_loss: 10431.5811\n",
      "Epoch [19960], val_loss: 10427.9111\n",
      "Epoch [19980], val_loss: 10425.0479\n",
      "Epoch [20000], val_loss: 10423.1260\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "lr = 10 ** -1\n",
    "history5 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the final validation loss of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 10423.1259765625}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = history5[-1]\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log the final validation loss to Jovian and commit the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Metrics logged.\n"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics(val_loss=val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"saurabhsjain/02-insurance-linear-regression\" on https://jovian.ml/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ml/saurabhsjain/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/saurabhsjain/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(filename= '02-insurance-linear', project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions using the trained model\n",
    "\n",
    "**Q: Complete the following function definition to make predictions on a single input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                # fill this\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"Input:\", input)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([63.0000,  0.0000, 30.8460,  0.0000,  0.0000,  3.0000])\n",
      "Target: tensor([16240.7100])\n",
      "Prediction: tensor([14825.0752])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[0]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([31.0000,  1.0000, 25.1569,  1.0000,  0.0000,  1.0000])\n",
      "Target: tensor([4960.6743])\n",
      "Prediction: tensor([6388.0127])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[10]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([61.0000,  0.0000, 24.3276,  0.0000,  0.0000,  2.0000])\n",
      "Target: tensor([28680.3164])\n",
      "Prediction: tensor([15039.4092])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[23]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you happy with your model's predictions? Try to improve them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I improved the model predictions by increasing the number of epochs and learning rate. Yes, I am happy with the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Step 6: Try another dataset & blog about it\n",
    "\n",
    "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to clean up & replicate this notebook (or [this one](https://jovian.ml/aakashns/housing-linear-minimal), or [this one](https://jovian.ml/aakashns/mnist-logistic-minimal) ) for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patters in machine learning from problem-specific details.\n",
    "\n",
    "Here are some sources to find good datasets:\n",
    "\n",
    "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n",
    "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n",
    "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n",
    "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n",
    "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "- https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n",
    "\n",
    "- Interesting title & subtitle\n",
    "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n",
    "- Downloading & exploring the data\n",
    "- Preparing the data for training\n",
    "- Creating a model using PyTorch\n",
    "- Training the model to fit the data\n",
    "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n",
    "- Making predictions using the model\n",
    "\n",
    "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n",
    "\n",
    "Don't forget to share your work on the forum: https://jovian.ml/forum/t/share-your-work-here-assignment-2/4931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name, environment=None)\n",
    "jovian.commit(project=project_name, environment=None) # try again, kaggle fails sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
